{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_CycleGAN_improved.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hlxUCTXB6Odx",
        "colab": {}
      },
      "source": [
        "# ! rm -r output/horse2zebra/*\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYlDPsDVu5c2",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrVFKv61i1bF",
        "colab_type": "code",
        "outputId": "6db23610-8156-4add-d29e-bbf7be720f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "! git clone https://github.com/Tymyan1/datasets.git\n",
        "! pip install tensorflow-gpu==2.0.0-alpha0\n",
        "! pip install tensorflow-addons\n",
        "! pip install zipfile36"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'datasets' already exists and is not an empty directory.\n",
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (1.12.0)\n",
            "Requirement already satisfied: zipfile36 in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyYDmQ3JixKY",
        "colab_type": "text"
      },
      "source": [
        "# Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utmClv0Tu78p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def linear_decayed_lr(epoch, lr, total_epochs, non_decayed_epochs=100):\n",
        "    if epoch < non_decayed_epochs:\n",
        "        return lr\n",
        "    return lr * (1 - 1 / (epoch - non_decayed_epochs) * (total_epochs - epoch))\n",
        "\n",
        "def tau_thres(input_, tau=0.1):\n",
        "    return np.where(input_ <= tau, 0, input_)\n",
        "\n",
        "# @tf.function\n",
        "def get_bg_map(att_map):\n",
        "    bg = (1 - mapa)\n",
        "    return bg\n",
        "\n",
        "# @tf.function\n",
        "def compose_img(att, fg, bg):\n",
        "    return (att * fg) + bg\n",
        "\n",
        "# @tf.function\n",
        "# def binarize(inp):\n",
        "#     zeros = tf.zeros_like(inp)\n",
        "#     ones = tf.ones_like(inp)\n",
        "#     return tf.where(tf.less(inp, 0), zeros, ones)\n",
        "     \n",
        "    \n",
        "\n",
        "# based on https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md\n",
        "# given the input img, plots all the activations along the whole network \n",
        "def show_flow(model, img):\n",
        "    # build a model\n",
        "    layer_outputs = [layer.output for layer in model.layers]\n",
        "    _model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "    \n",
        "    activations = _model.predict(img)\n",
        "    \n",
        "    path = 'layer_imgs/genA/'\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    \n",
        "    layer_names = []\n",
        "    for layer in model.layers:\n",
        "        layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
        "\n",
        "    images_per_row = 16\n",
        "    i = 0\n",
        "    for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
        "        n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
        "        size = layer_activation.shape[1] # The feature map has shape (1, size, size, n_features).\n",
        "        n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
        "        n_cols = max(n_cols, 1)\n",
        "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "        \n",
        "        feature_counter = 0\n",
        "        for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
        "            for row in range(images_per_row):\n",
        "                if feature_counter >=  n_features:\n",
        "                    break\n",
        "                \n",
        "                channel_image = layer_activation[0,\n",
        "                                                 :, :,\n",
        "                                                 col * images_per_row + row]\n",
        "                channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "                display_grid[col * size : (col + 1) * size, # Displays the grid\n",
        "                             row * size : (row + 1) * size] = channel_image\n",
        "                feature_counter += 1\n",
        "        scale = 1. / size\n",
        "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                            scale * display_grid.shape[0]))\n",
        "        plt.title(layer_name)\n",
        "        plt.grid(False)\n",
        "        \n",
        "       \n",
        "        # plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "        plt.savefig('{}/{:03d}_{}.png'.format(path, i, layer_name), aspect='auto')#, cmap='viridis')\n",
        "        i += 1\n",
        "    print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkJhQ09FkMxo",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmxcKKmokb_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Data_Loader():\n",
        "    ROOT_PATH = './datasets/'\n",
        "\n",
        "    def __init__(self, name, batch_size, img_shape=(256,256,3), patch=None):\n",
        "        self.name = name\n",
        "        self.batch_size = batch_size\n",
        "        self.img_dims = (patch + (3,)) if patch else img_shape\n",
        "\n",
        "        # load in the data\n",
        "        pathsA = glob(Data_Loader.ROOT_PATH + name + '/trainM/*')\n",
        "        pathsB = glob(Data_Loader.ROOT_PATH + name + '/trainB/*')\n",
        "        countA = len(pathsA)\n",
        "        countB = len(pathsB)\n",
        "        self.n_batches = int(min(countA, countB) / self.batch_size)\n",
        "        \n",
        "        self.dsA = tf.data.Dataset.from_tensor_slices(pathsA)\n",
        "        self.dsB = tf.data.Dataset.from_tensor_slices(pathsB)\n",
        "        \n",
        "        self.dsA = self.dsA.map(lambda img: _load_and_preprocess_image(img, img_dims=[self.img_dims[0], self.img_dims[1]]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        self.dsB = self.dsB.map(lambda img: _load_and_preprocess_image(img, img_dims=[self.img_dims[0], self.img_dims[1]]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "        self.dsA = self.dsA.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=countA))\n",
        "        self.dsB = self.dsB.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=countB))\n",
        "        \n",
        "        self.dsA = self.dsA.batch(batch_size)\n",
        "        self.dsB = self.dsB.batch(batch_size)\n",
        "        \n",
        "        self.dsA = self.dsA.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        self.dsB = self.dsB.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "        self.itA = iter(self.dsA)\n",
        "        self.itB = iter(self.dsB)\n",
        "        \n",
        "        self.it_counter = 0\n",
        "        \n",
        "        samples = {\n",
        "            'horse2zebra': ('datasets/horse2zebra/testA/n02381460_120.jpg', 'datasets/horse2zebra/testB/n02391049_1880.jpg'),\n",
        "#             'sim2larvae': ('datasets/sim2larvae/testA/view3151.png', 'datasets/sim2larvae/testB/img_331_8.png')\n",
        "            'sim2larvae': ('datasets/sim2larvae/testM/view3151.png', 'datasets/sim2larvae/testB/img_331_8.png')\n",
        "        }\n",
        "\n",
        "        self.samples = np.expand_dims(_load_and_preprocess_image(samples[name][0], [self.img_dims[0], self.img_dims[1]]), axis=0), \\\n",
        "                       np.expand_dims(_load_and_preprocess_image(samples[name][1], [self.img_dims[0], self.img_dims[1]]), axis=0)\n",
        "        \n",
        "        \n",
        "    def load_batch(self):\n",
        "        self.it_counter += 1\n",
        "        return next(self.itA), next(self.itB)\n",
        "\n",
        "    def batches_left(self):\n",
        "        return self.it_counter % self.n_batches\n",
        "    \n",
        "    def sample_batch(self):\n",
        "        return self.samples\n",
        "    \n",
        "def _load_and_preprocess_image(path, img_dims):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, img_dims)\n",
        "    image = (image / 127.5) - 1\n",
        "    return image    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvxPZat9kgAp",
        "colab_type": "text"
      },
      "source": [
        "# Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4yPfFpkoaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Pad(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, paddings, mode='CONSTANT', constant_values=0, **kwargs):\n",
        "        super(Pad, self).__init__(**kwargs)\n",
        "        self.paddings = paddings\n",
        "        self.mode = mode\n",
        "        self.constant_values = constant_values\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.pad(inputs, self.paddings, mode=self.mode, constant_values=self.constant_values)\n",
        "\n",
        "\n",
        "class TauThreshold(tf.keras.layers.Layer):\n",
        "    def __init__(self, tau=0.1, **kwargs):\n",
        "        super(TauThreshold, self).__init__(**kwargs)\n",
        "        self.tau = tau\n",
        "\n",
        "    def call(self, input_):\n",
        "        zeros = tf.zeros_like(input_)\n",
        "        return tf.where(tf.less(input_, self.tau), zeros, input_)\n",
        "    \n",
        "\n",
        "class UpSample(tf.keras.layers.Layer):\n",
        "    def __init__(self, size, **kwargs):\n",
        "        super(UpSample, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, input_):\n",
        "        return tf.image.resize(input_, size=self.size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooP3ztummEQj",
        "colab_type": "text"
      },
      "source": [
        "# Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSU6MTmSmKNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "class LinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    Linear learning rate decay down to 0 applied after decay_offset_steps steps\n",
        "    \"\"\"\n",
        "    def __init__(self, lr, total_steps, decay_offset_steps):\n",
        "        self.lr = lr\n",
        "        self.total_steps = total_steps\n",
        "        self.decay_offset_steps = decay_offset_steps\n",
        "        #1.13 version bellow\n",
        "#         self.current_learning_rate = tf.Variable(initial_value=lr, trainable=False, dtype=tf.float32)\n",
        "\n",
        "#     def __call__(self, step):\n",
        "#         self.current_learning_rate.assign(tf.cond(\n",
        "#             step >= self.decay_offset_steps,\n",
        "#             true_fn=lambda: self.lr * (\n",
        "#                         1 - 1 / (self.total_steps - self.decay_offset_steps) * (step - self.decay_offset_steps)),\n",
        "#             false_fn=lambda: self.lr\n",
        "#         ))\n",
        "#         return self.current_learning_rate\n",
        "\n",
        "    @tf.function\n",
        "    def __call__(self, step):\n",
        "        if step >= self.decay_offset_steps:\n",
        "            return self.lr * (1 - 1 / (self.total_steps - self.decay_offset_steps) * (step - self.decay_offset_steps))\n",
        "        return self.lr\n",
        "    \n",
        "    def get_config(self):\n",
        "        return json.dumps({\n",
        "            'lr': self.lr,\n",
        "            'total_steps': self.total_steps,\n",
        "            'decay_offset_steps': self.decay_offset_steps\n",
        "#             'current_learning_rate': self.current_learning_rate\n",
        "        })\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvnbBoHunB7a",
        "colab_type": "text"
      },
      "source": [
        "# ItemPool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7kYlpzhnJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import stack\n",
        "import random\n",
        "\n",
        "class ItemPool(object):\n",
        "    def __init__(self, size=50):\n",
        "        self.size = size\n",
        "        self.queue = []\n",
        "\n",
        "    def call(self, elem):\n",
        "        if len(self.queue) < self.size:\n",
        "            self.queue.append(elem)\n",
        "            return elem\n",
        "        else:\n",
        "            if random.random() < .5:\n",
        "                # replace a random element with the new one\n",
        "                index = random.randint(0, self.size-1)\n",
        "                tmp = self.queue[index]\n",
        "                self.queue[index] = elem\n",
        "                return tmp\n",
        "            else:\n",
        "                # just return the current element without adding\n",
        "                return elem\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkKQRfFdnXit",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gwqLZVPnZr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def _g_conv_layer(input_, filters, filter_size, norm='instance', strides=2, pad='VALID', relu=True):\n",
        "#     input_ = tf.cast(input_, tf.float32)\n",
        "    conv = tf.keras.layers.Conv2D(filters, kernel_size=filter_size, padding=pad, strides=strides)(input_)\n",
        "    if norm == 'instance':\n",
        "        conv = tfa.layers.InstanceNormalization()(conv)\n",
        "    if relu == True:\n",
        "        conv = tf.keras.layers.ReLU()(conv)\n",
        "    return conv\n",
        "\n",
        "def _g_res_block(input_, norm='instance'):\n",
        "    filters = input_.shape[-1]\n",
        "    out = Pad([[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')(input_)\n",
        "#     out = input_\n",
        "    out = _g_conv_layer(out, filters, norm=norm, filter_size=3, strides=1, pad='VALID', relu=True)\n",
        "    out = tf.keras.layers.ReLU()(out)\n",
        "\n",
        "    out = Pad([[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')(out)\n",
        "    out = _g_conv_layer(out, filters, norm=norm, filter_size=3, strides=1, pad='VALID', relu=False)\n",
        "    out = tf.keras.layers.add([input_, out])\n",
        "    out = tf.keras.layers.ReLU()(out) #TODO enable??\n",
        "    return out\n",
        "\n",
        "def _g_deconv_layer(input_, filters, filter_size, pad='SAME', norm='instance'):\n",
        "    size = (input_.shape[1] * 2,) + (input_.shape[2] * 2,)\n",
        "    out = UpSample(size=size)(input_)\n",
        "    out = Pad([[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')(out)\n",
        "    out = tf.keras.layers.Conv2D(input_.shape[-1], kernel_size=3, padding='VALID', strides=1)(out)\n",
        "#     out = tf.keras.layers.Conv2DTranspose(filters, kernel_size=filter_size, strides=2, padding=pad)(input_)\n",
        "    if norm == 'instance':\n",
        "        out = tfa.layers.InstanceNormalization()(out)\n",
        "    out = tf.keras.layers.ReLU()(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_generator(input_shape, name):\n",
        "\n",
        "    norm = 'instance'\n",
        "    \n",
        "    model = input_ = tf.keras.layers.Input(shape=input_shape)\n",
        "    \n",
        "    # c7s1-32-R\n",
        "    model = Pad([[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')(model)\n",
        "    model = _g_conv_layer(model, 32, filter_size=7, strides=1, pad='VALID', norm=norm)\n",
        "    \n",
        "    # upsampling\n",
        "    # c3s2-64-R\n",
        "    model = _g_conv_layer(model, filters=64, filter_size=3, strides=2, pad='SAME', norm=norm)\n",
        "    # c3s2-128-R\n",
        "    model = _g_conv_layer(model, filters=128, filter_size=3, strides=2, pad='SAME', norm=norm)\n",
        "    \n",
        "    # residual blocks\n",
        "    # r128 * 9\n",
        "    for i in range(9):\n",
        "        model = _g_res_block(model)\n",
        "    \n",
        "    \n",
        "    # downsampling\n",
        "    # tc64s2\n",
        "    model = _g_deconv_layer(model, 64, 3, pad='SAME', norm=norm)\n",
        "    # tc32s2\n",
        "    model = _g_deconv_layer(model, 32, 3, pad='SAME', norm=norm)\n",
        "    \n",
        "    # c3s1-3-T\n",
        "    model = Pad([[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')(model)\n",
        "    model = tf.keras.layers.Conv2D(filters=3, kernel_size=7, strides=1, padding='VALID')(model) # 3 img channels\n",
        "    model = tf.keras.layers.Activation('tanh')(model)\n",
        "\n",
        "    return tf.keras.Model(input_, model, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgSPzQktt_HX",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWD3Icg5uAha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def _discriminator_layer(input_, input_nn, filters, filter_size, strides, norm='instance'):\n",
        "    \"\"\"\n",
        "    :param input_:\n",
        "    :param filters:\n",
        "    :param filter_size:\n",
        "    :param norm:\n",
        "    :return: Two discriminator blocks (with/without normalization layers) with shared layers\n",
        "    \"\"\"\n",
        "#     input_ = tf.cast(input_, tf.float32)\n",
        "    layer = layer_nn = tf.keras.layers.Conv2D(filters, kernel_size=filter_size, strides=strides, padding='SAME')(input_)\n",
        "    if norm=='instance':\n",
        "        layer = tfa.layers.InstanceNormalization()(layer)\n",
        "    relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "    layer = relu(layer)\n",
        "    layer_nn = relu(layer_nn)\n",
        "\n",
        "    return layer, layer_nn\n",
        "\n",
        "\n",
        "def build_discriminator(input_shape, name):\n",
        "    \"\"\"\n",
        "    :param input_shape:\n",
        "    :return: Two discriminators(with/without normalization) with shared layers\n",
        "    \"\"\"\n",
        "    \n",
        "    input_ = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # c4s2-64-LR\n",
        "    dis, dis_nn = _discriminator_layer(input_, input_, 64, filter_size=4, strides=2, norm='instance')\n",
        "    # c4s2-128-LR\n",
        "    dis, dis_nn = _discriminator_layer(dis, dis_nn, 128, filter_size=4, strides=2, norm='instance')\n",
        "    # c4s2-256-LR\n",
        "    dis, dis_nn = _discriminator_layer(dis, dis_nn, 256, filter_size=4, strides=2, norm='instance')\n",
        "    # c4s1-512-LR\n",
        "    dis, dis_nn = _discriminator_layer(dis, dis_nn, 512, filter_size=4, strides=1, norm='instance')\n",
        "    \n",
        "    # c4s1-1\n",
        "    last_layer = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='SAME')\n",
        "    dis = last_layer(dis)\n",
        "    dis_nn = last_layer(dis_nn)\n",
        "\n",
        "    return tf.keras.Model(input_, dis, name=name), tf.keras.Model(input_, dis_nn, name=name+'_nn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13tDEgOtuQOw",
        "colab_type": "text"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDyywD_6uR7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def _a_conv_layer(input_, filters, filter_size, norm='instance', strides=2, pad='SAME', relu=True):\n",
        "    conv = tf.keras.layers.Conv2D(filters, kernel_size=filter_size, padding=pad, strides=strides)(input_)\n",
        "    if norm == 'instance':\n",
        "        conv = tfa.layers.InstanceNormalization()(conv)\n",
        "    if relu == True:\n",
        "        conv = tf.keras.layers.ReLU()(conv)\n",
        "    return conv\n",
        "\n",
        "def _a_res_block(input_, norm='instance'):\n",
        "    filters = input_.shape[-1]\n",
        "   \n",
        "    out = Pad([[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')(input_)\n",
        "#     out = input_\n",
        "    out = _a_conv_layer(out, filters, norm=norm, filter_size=3, strides=1, pad='VALID', relu=False)\n",
        "    out = tf.keras.layers.ReLU()(out)\n",
        "\n",
        "    out = Pad([[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')(out)\n",
        "    out = _a_conv_layer(out, filters, norm=norm, filter_size=3, strides=1, pad='VALID', relu=False)\n",
        "    out = tf.keras.layers.add([input_, out])\n",
        "    out = tf.keras.layers.ReLU()(out) #TODO enable??\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_attention_net(input_shape, name):\n",
        "    size = (input_shape[0],) + (input_shape[1],)\n",
        "    # print(size)\n",
        "    model = input_ = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # c7s1-32-R\n",
        "    model = _a_conv_layer(model, filters=32, filter_size=7,  strides=1)\n",
        "    # c3s2-64-R\n",
        "    model = _a_conv_layer(model, filters=64, filter_size=3)\n",
        "    # r64\n",
        "    model = _a_res_block(model, 3)\n",
        "    # up2\n",
        "    model = tf.keras.layers.UpSampling2D(size=2, interpolation='nearest')(model)\n",
        "#     model = UpSample(size=size)(model)\n",
        "    # c3s1-64-R\n",
        "    model = _a_conv_layer(model, strides=1, filters=64, filter_size=3)\n",
        "    # up2\n",
        "    model = tf.keras.layers.UpSampling2D(size=2, interpolation='nearest')(model)\n",
        "#     model = UpSample(size=size)(model) #TODO enable??\n",
        "    # c3s1-32-R\n",
        "    model = _a_conv_layer(model, strides=2, filters=32, filter_size=3) #TODO change to stride=1\n",
        "    # c7s1-1-S\n",
        "    model = _a_conv_layer(model, strides=1, filters=1, filter_size=7, relu=False)\n",
        "    model = tf.keras.activations.sigmoid(model)\n",
        "\n",
        "    return tf.keras.Model(input_, model, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvnMI_PA1PGT",
        "colab_type": "text"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JllNkB-G1I-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cyclic_loss(gen_img, real_img): \n",
        "    return tf.reduce_mean(tf.abs(real_img - gen_img))\n",
        "\n",
        "def adversarial_loss(prediction_on_real, prediction_on_fake, prediction_on_cyclic): \n",
        "    return (2 * tf.reduce_mean(tf.math.squared_difference(prediction_on_real, 1))) + \\\n",
        "                tf.reduce_mean(tf.math.squared_difference(prediction_on_fake, 0)) + \\\n",
        "                tf.reduce_mean(tf.math.squared_difference(prediction_on_cyclic, 0))\n",
        "\n",
        "def generator_adversarial_loss(dis_prediction_of_img):\n",
        "    return tf.reduce_mean(tf.math.squared_difference(dis_prediction_of_img, 1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm9BMYLYugbo",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3fb5OqQuh2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "class Model2(tf.keras.Model):\n",
        "    def __init__(self, epochs, data_loader, patch=(70,70), load_from_checkpoint=True, use_att=True, tau=0.1, attention_epochs_threshold=30, lr_d=0.002, lr_g=0.005, beta1=0.5, epoch_decay=100, pool_size=50):\n",
        "        super(tf.keras.Model, self).__init__()\n",
        "        self.data = data_loader\n",
        "        self.weights_path = 'output/' + self.data.name + '/weights.h5'\n",
        "        self.epochs = epochs\n",
        "        self.pool_size = pool_size\n",
        "        self.use_att = use_att\n",
        "        self.tau = tau\n",
        "        self.att_epoch_thresh = attention_epochs_threshold\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cyclic = 10.0  # Cycle-consistency loss weight\n",
        "        self.lambda_id = 0.1 * self.lambda_cyclic  # Identity loss weight\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        # self.disc_patch = (self.data.img_dims[0], self.data.img_dims[1], 1)\n",
        "        self.disc_patch = (32,32,1) #TODO this is just a quickfix\n",
        "\n",
        "        # build models\n",
        "        # print(self.data.img_dims)\n",
        "        self.disA, self.disA_no_norm = build_discriminator(self.data.img_dims, 'disA')\n",
        "        self.disB, self.disB_no_norm = build_discriminator(self.data.img_dims, 'disB')\n",
        "      \n",
        "        self.genA = build_generator(self.data.img_dims, 'genA')\n",
        "        self.genB = build_generator(self.data.img_dims, 'genB')\n",
        "        self.attA = build_attention_net(self.data.img_dims, 'attA')\n",
        "        self.attB = build_attention_net(self.data.img_dims, 'attB')\n",
        "        \n",
        "\n",
        "        # lr schedulers\n",
        "        _total_steps = self.data.batches_left() * self.epochs\n",
        "        _decay_offset_steps = epoch_decay * self.epochs\n",
        "        lr_sched_d = LinearDecay(lr_d, _total_steps, _decay_offset_steps)\n",
        "        lr_sched_g = LinearDecay(lr_g, _total_steps, _decay_offset_steps)\n",
        "        \n",
        "        # set the optimizers\n",
        "        self.optimizer_d = tf.keras.optimizers.Adam(learning_rate=lr_sched_d, beta_1=beta1)\n",
        "        self.optimizer_g = tf.keras.optimizers.Adam(learning_rate=lr_sched_g, beta_1=beta1)\n",
        "\n",
        "        #TODO move?\n",
        "        if load_from_checkpoint:\n",
        "            self.load_weights(self.weights_path)\n",
        "\n",
        "        # inputs\n",
        "        imgA = tf.keras.layers.Input(shape=self.data.img_dims)\n",
        "        imgB = tf.keras.layers.Input(shape=self.data.img_dims)\n",
        "\n",
        "        # get attention maps\n",
        "        # attnMapA = toZeroThreshold(AttnA(realA))\n",
        "        attA = TauThreshold(self.tau)(self.attA(imgA))\n",
        "        attB = TauThreshold(self.tau)(self.attB(imgB))\n",
        "        \n",
        "        # fgA = attnMapA * realA\n",
        "        imgA_fg = imgA * attA\n",
        "        imgB_fg = imgB * attB\n",
        "        \n",
        "        # bgA = (1 - attnMapA) * realA\n",
        "        imgA_bg = (1 - attA) * imgA\n",
        "        imgB_bg = (1 - attB) * imgB\n",
        "        \n",
        "        # IMAGE TRANSLATION\n",
        "        # genB = genA2B(fgA) \n",
        "        fakeA_fg = self.genA(imgB_fg)\n",
        "        fakeB_fg = self.genB(imgA_fg)\n",
        "        \n",
        "        # fakeB = (attnMapA * genB) + bgA\n",
        "        fakeB = fakeB_fg * attA + imgA_bg # s'\n",
        "        fakeA = fakeA_fg * attB + imgB_bg\n",
        "\n",
        "        # CYCLIC TRANSLATION\n",
        "        # get attention maps\n",
        "        # attnMapfakeB = toZeroThreshold(AttnB(fakeB))\n",
        "        attA_fake = TauThreshold(self.tau)(self.attA(fakeA))\n",
        "        attB_fake = TauThreshold(self.tau)(self.attB(fakeB))\n",
        "\n",
        "        # get the foreground\n",
        "        # fgfakeB = attnMapfakeB * fakeB\n",
        "        fakeA_fg = fakeA * attA_fake\n",
        "        fakeB_fg = fakeB * attB_fake\n",
        "\n",
        "        # get the background\n",
        "        # bgfakeB = (1 - attnMapfakeB) * fakeB\n",
        "        cyclicA_bg = (1 - attA_fake) * fakeA\n",
        "        cyclicB_bg = (1 - attB_fake) * fakeB\n",
        "        \n",
        "        # genA_ = genB2A(fgfakeB)\n",
        "        cyclicA_fg = self.genA(fakeB_fg)\n",
        "        cyclicB_fg = self.genB(fakeA_fg)\n",
        "        \n",
        "        # combine\n",
        "        # A_ = (attnMapfakeB * genA_) + bgfakeB\n",
        "        cyclicA = attB_fake * cyclicA_fg + cyclicB_bg # s''\n",
        "        cyclicB = attA_fake * cyclicB_fg + cyclicA_bg # s''\n",
        "        \n",
        "        # compile basic discriminators\n",
        "        self.disA.compile(loss='mse', optimizer=self.optimizer_d, metrics=['accuracy'])\n",
        "        self.disB.compile(loss='mse', optimizer=self.optimizer_d, metrics=['accuracy'])\n",
        "        self.disA_no_norm.compile(loss='mse', optimizer=self.optimizer_d, metrics=['accuracy'])\n",
        "        self.disB_no_norm.compile(loss='mse', optimizer=self.optimizer_d, metrics=['accuracy'])\n",
        "\n",
        "        #TODO attention to identity mappings? probably not?\n",
        "        # Identity mapping of images\n",
        "        # img1_id = self.gen2(img1)\n",
        "        # img2_id = self.gen1(img2)\n",
        "\n",
        "        # combined model only trains generators (and attention)\n",
        "        self.disA.trainable = False\n",
        "        self.disB.trainable = False\n",
        "        self.disA_no_norm.trainable = False\n",
        "        self.disB_no_norm.trainable = False\n",
        "\n",
        "        # discriminate the fake images\n",
        "        # stage 1 - whole image with normalisation\n",
        "        validityA_stage1 = self.disA(fakeA)\n",
        "        validityB_stage1 = self.disB(fakeB)\n",
        "\n",
        "        cyclic_valA_stage1 = self.disA(cyclicA)\n",
        "        cyclic_valB_stage1 = self.disB(cyclicB)\n",
        "        \n",
        "        # use dis without normalisation on the foreground only\n",
        "        validityA_stage2 = self.disA_no_norm(fakeA_fg)\n",
        "        validityB_stage2 = self.disB_no_norm(fakeB_fg)\n",
        "        \n",
        "        cyclic_valA_stage2 = self.disA_no_norm(cyclicA_fg)\n",
        "        cyclic_valB_stage2 = self.disB_no_norm(cyclicB_fg)\n",
        "\n",
        "        # build and compile combined model for stage 1\n",
        "        self.combined_model1 = tf.keras.Model(inputs=[imgA, imgB],\n",
        "                                             outputs=[validityA_stage1, validityB_stage1, cyclic_valA_stage1, cyclic_valB_stage1, cyclicA, cyclicB])#, img1_id, img2_id])\n",
        "        self.combined_model1.compile(optimizer=self.optimizer_g,\n",
        "                                    loss=['mse', 'mse', 'mse', 'mse', 'mae', 'mae'])#, 'mae', 'mae'],\n",
        "                                    #loss_weights=[1, 1, self.lambda_cyclic, self.lambda_cyclic])#, self.lambda_id, self.lambda_id])\n",
        "\n",
        "        # no more attention training in stage 2\n",
        "        self.attA.trainable = False\n",
        "        self.attB.trainable = False\n",
        "\n",
        "        # build and compile combined model for stage 2\n",
        "        self.combined_model2 = tf.keras.Model(inputs=[imgA, imgB],\n",
        "                                                    outputs=[validityA_stage2, validityB_stage2, cyclic_valA_stage2, cyclic_valB_stage2, cyclicA, cyclicB])#, img1_id, img2_id])\n",
        "        self.combined_model2.compile(optimizer=self.optimizer_g,\n",
        "                                           loss=['mse', 'mse', 'mse', 'mse', 'mae', 'mae'])#, 'mae', 'mae'],\n",
        "                                           #loss_weights=[1, 1, self.lambda_cyclic, self.lambda_cyclic])#, self.lambda_id, self.lambda_id])\n",
        "\n",
        "\n",
        "    def train(self, sample_interval=5):\n",
        "        \n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # fake img pools for dis\n",
        "        self.fakeA_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeB_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeA_cyclic_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeB_cyclic_pool = ItemPool(size=self.pool_size)\n",
        "        \n",
        "        self.fakeA_fg_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeB_fg_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeA_cyclic_fg_pool = ItemPool(size=self.pool_size)\n",
        "        self.fakeB_cyclic_fg_pool = ItemPool(size=self.pool_size)\n",
        "        \n",
        "        # adversarial loss ground truth\n",
        "        valid = np.ones((self.data.batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((self.data.batch_size,) + self.disc_patch)\n",
        "\n",
        "        is_after_att_thres = False\n",
        "       \n",
        "        for epoch in range(1, self.epochs+1):\n",
        "            self.cur_epoch = epoch\n",
        "            if epoch >= self.att_epoch_thresh:\n",
        "                is_after_att_thres = True\n",
        "\n",
        "            for batch_i in range(1, self.data.n_batches+1):\n",
        "                imgA, imgB = self.data.load_batch()\n",
        "\n",
        "                # GENERATE IMAGES\n",
        "                # get attention maps\n",
        "                # attnMapA = toZeroThreshold(AttnA(realA))\n",
        "                attA = tau_thres(self.attA.predict(imgA), tau=self.tau)\n",
        "                attB = tau_thres(self.attB.predict(imgB), tau=self.tau)\n",
        "\n",
        "                # fgA = attnMapA * realA\n",
        "                imgA_fg = imgA * attA\n",
        "                imgB_fg = imgB * attB\n",
        "\n",
        "                # bgA = (1 - attnMapA) * realA\n",
        "                imgA_bg = (1 - attA) * imgA\n",
        "                imgB_bg = (1 - attB) * imgB\n",
        "\n",
        "                # IMAGE TRANSLATION\n",
        "                # genB = genA2B(fgA) \n",
        "                fakeA_fg = self.genA.predict(imgB_fg)\n",
        "                fakeB_fg = self.genB.predict(imgA_fg)\n",
        "\n",
        "                # fakeB = (attnMapA * genB) + bgA\n",
        "                fakeB = fakeB_fg * attA + imgA_bg # s'\n",
        "                fakeA = fakeA_fg * attB + imgB_bg\n",
        "\n",
        "                # CYCLIC TRANSLATION\n",
        "                # get attention maps\n",
        "                # attnMapfakeB = toZeroThreshold(AttnB(fakeB))\n",
        "                attA_fake = tau_thres(self.attA.predict(fakeA), tau=self.tau)\n",
        "                attB_fake = tau_thres(self.attB.predict(fakeB), tau=self.tau)\n",
        "\n",
        "                # get the foreground\n",
        "                # fgfakeB = attnMapfakeB * fakeB\n",
        "                fakeA_fg = fakeA * attA_fake\n",
        "                fakeB_fg = fakeB * attB_fake\n",
        "\n",
        "                # get the background\n",
        "                # bgfakeB = (1 - attnMapfakeB) * fakeB\n",
        "                cyclicA_bg = (1 - attA_fake) * fakeA\n",
        "                cyclicB_bg = (1 - attB_fake) * fakeB\n",
        "\n",
        "                # genA_ = genB2A(fgfakeB)\n",
        "                cyclicA_fg = self.genA.predict(fakeB_fg)\n",
        "                cyclicB_fg = self.genB.predict(fakeA_fg)\n",
        "\n",
        "                # combine\n",
        "                # A_ = (attnMapfakeB * genA_) + bgfakeB\n",
        "                cyclicA = attB_fake * cyclicA_fg + cyclicB_bg # s''\n",
        "                cyclicB = attA_fake * cyclicB_fg + cyclicA_bg # s''\n",
        "\n",
        "                # TRAIN DISCRIMINATORS\n",
        "                if is_after_att_thres:\n",
        "                     # pool management\n",
        "                    fakeA_fg_pool = self.fakeA_fg_pool.call(fakeA_fg)\n",
        "                    fakeB_fg_pool = self.fakeB_fg_pool.call(fakeB_fg)\n",
        "                    cyclicA_fg_pool = self.fakeA_cyclic_fg_pool.call(cyclicA_fg)\n",
        "                    cyclicB_fg_pool = self.fakeB_cyclic_fg_pool.call(cyclicB_fg)\n",
        "\n",
        "                    # discriminate only on the foreground\n",
        "                    # real losses\n",
        "                    real_predA = self.disA_no_norm.train_on_batch(imgA_fg, valid, sample_weight=np.array([2]))\n",
        "                    real_predB = self.disB_no_norm.train_on_batch(imgB_fg, valid, sample_weight=np.array([2]))\n",
        "\n",
        "\n",
        "                    # fake losses\n",
        "                    fake_predA = self.disA_no_norm.train_on_batch(fakeA_fg, fake)\n",
        "                    fake_predB = self.disB_no_norm.train_on_batch(fakeB_fg, fake)\n",
        "                    \n",
        "                    # fake cyclic losses\n",
        "                    cyclic_fake_predA = self.disA_no_norm.train_on_batch(cyclicA_fg, fake)\n",
        "                    cyclic_fake_predB = self.disB_no_norm.train_on_batch(cyclicB_fg, fake)\n",
        "                else:\n",
        "                    # pool management\n",
        "                    fakeA_pool = self.fakeA_pool.call(fakeA)\n",
        "                    fakeB_pool = self.fakeB_pool.call(fakeB)\n",
        "                    cyclicA_pool = self.fakeA_cyclic_pool.call(cyclicA)\n",
        "                    cyclicB_pool = self.fakeB_cyclic_pool.call(cyclicB)\n",
        "                    \n",
        "                    # real loss\n",
        "                    real_predA = self.disA.train_on_batch(imgA, valid, sample_weight=np.array([2]))\n",
        "                    real_predB = self.disB.train_on_batch(imgB, valid, sample_weight=np.array([2]))\n",
        "                \n",
        "                    # fake loss\n",
        "                    fake_predA = self.disA.train_on_batch(fakeA, fake)\n",
        "                    fake_predB = self.disB.train_on_batch(fakeB, fake) \n",
        "                    \n",
        "                    # cyclic fake loss\n",
        "                    cyclic_fake_predA = self.disA.train_on_batch(cyclicA, fake)\n",
        "                    cyclic_fake_predB = self.disB.train_on_batch(cyclicB, fake)\n",
        "\n",
        "                # dis losses total\n",
        "                disA_loss = adversarial_loss(real_predA, fake_predA, cyclic_fake_predA)\n",
        "                disB_loss = adversarial_loss(real_predB, fake_predB, cyclic_fake_predA)\n",
        "                dis_total_loss = disA_loss + disB_loss\n",
        "\n",
        "                \n",
        "                # TRAIN GENERATORS\n",
        "                if is_after_att_thres:\n",
        "                    gen_loss = self.combined_model2.train_on_batch([imgA, imgB],\n",
        "                                                                         [valid, valid, valid, valid, imgA, imgB])#, imgs1, imgs2])\n",
        "                else:\n",
        "                    gen_loss = self.combined_model1.train_on_batch([imgA, imgB],\n",
        "                                                                      [valid, valid, valid, valid, imgA, imgB])#, imgs1, imgs2])\n",
        "\n",
        "    \n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "                if batch_i % 50 == 0:\n",
        "                    print(\n",
        "                        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %05f] time: %s \" \\\n",
        "                        % (epoch, self.epochs,\n",
        "                           batch_i, self.data.n_batches,\n",
        "                           dis_total_loss,\n",
        "                           gen_loss[0],\n",
        "                           elapsed_time))\n",
        "\n",
        "\n",
        "                mod_name = 'full_orig'\n",
        "                os.makedirs('model/%s/%s' % (self.data.name, mod_name), exist_ok=True)\n",
        "#                 save_time = datetime.datetime.now()\n",
        "            \n",
        "                if batch_i % 1000 == 0:\n",
        "                    self.sample_images(epoch, batch_i)\n",
        "                    it = batch_i // 1000\n",
        "                    self.combined_model1.save_weights('model/{}/{}/combined1_{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it))\n",
        "                    self.combined_model2.save_weights('model/{}/{}/combined2_{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it))\n",
        "                    self.disA.save_weights('model/{}/{}/disA{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it))\n",
        "                    self.disB.save_weights('model/{}/{}/disB{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it))\n",
        "        #                 self.disA_no_norm.save_weights('model/%s/%s/disA_nn%d' % (self.data.name, mod_name, epoch))\n",
        "        #                 self.disB_no_norm.save_weights('model/%s/%s/disB_nn%d' % (self.data.name, mod_name, epoch))\n",
        "        #                 print('Saving models took: ' + str(datetime.datetime.now() - save_time))\n",
        "                    self.save_things_to_drive(epoch, it)\n",
        "\n",
        "\n",
        "    def load_weights(self, comb1_p, comb2_p, disA_p, disB_p):\n",
        "        self.combined_model1.load_weights(comb1_p)\n",
        "        self.combined_model2.load_weights(comb2_p)\n",
        "        self.disA.load_weights(disA_p)\n",
        "        self.disB.load_weights(disB_p)\n",
        "        \n",
        "        \n",
        "    def sample_images(self, epoch, batch_i):\n",
        "#         os.makedirs('images/%s' % self.data.name, exist_ok=True)\n",
        "        r, c = 2, 9\n",
        "\n",
        "        imgA, imgB = self.data.sample_batch()\n",
        "\n",
        "        # attnMapA = toZeroThreshold(AttnA(realA))\n",
        "        attA = TauThreshold(self.tau)(self.attA.predict(imgA))\n",
        "        attB = TauThreshold(self.tau)(self.attB.predict(imgB))\n",
        "        \n",
        "        # fgA = attnMapA * realA\n",
        "        imgA_fg = imgA * attA\n",
        "        imgB_fg = imgB * attB\n",
        "        \n",
        "        # bgA = (1 - attnMapA) * realA\n",
        "        imgA_bg = (1 - attA) * imgA\n",
        "        imgB_bg = (1 - attB) * imgB\n",
        "        \n",
        "        # IMAGE TRANSLATION\n",
        "        # genB = genA2B(fgA) \n",
        "        fakeA_fg = self.genA.predict(imgB_fg)\n",
        "        fakeB_fg = self.genB.predict(imgA_fg)\n",
        "        \n",
        "        # fakeB = (attnMapA * genB) + bgA\n",
        "        fakeB = fakeB_fg * attA + imgA_bg # s'\n",
        "        fakeA = fakeA_fg * attB + imgB_bg\n",
        "\n",
        "        # CYCLIC TRANSLATION\n",
        "        # get attention maps\n",
        "        # attnMapfakeB = toZeroThreshold(AttnB(fakeB))\n",
        "        attA_fake = TauThreshold(self.tau)(self.attA.predict(fakeA))\n",
        "        attB_fake = TauThreshold(self.tau)(self.attB.predict(fakeB))\n",
        "\n",
        "        # get the foreground\n",
        "        # fgfakeB = attnMapfakeB * fakeB\n",
        "        fakeA_fg = fakeA * attA_fake\n",
        "        fakeB_fg = fakeB * attB_fake\n",
        "\n",
        "        # get the background\n",
        "        # bgfakeB = (1 - attnMapfakeB) * fakeB\n",
        "        cyclicA_bg = (1 - attA_fake) * fakeA\n",
        "        cyclicB_bg = (1 - attB_fake) * fakeB\n",
        "        \n",
        "        # genA_ = genB2A(fgfakeB)\n",
        "        cyclicA_fg = self.genA.predict(fakeB_fg)\n",
        "        cyclicB_fg = self.genB.predict(fakeA_fg)\n",
        "        \n",
        "        # combine\n",
        "        # A_ = (attnMapfakeB * genA_) + bgfakeB\n",
        "        cyclicA = attB_fake * cyclicA_fg + cyclicB_bg # s''\n",
        "        cyclicB = attA_fake * cyclicB_fg + cyclicA_bg # s''\n",
        "        \n",
        "        attA = 2 * (attA - 0.5)\n",
        "        attB = 2 * (attB - 0.5)\n",
        "        attA_fake = 2 * (attA_fake - 0.5)\n",
        "        attB_fake = 2 * (attB_fake - 0.5)\n",
        "        \n",
        "        attA = np.concatenate([attA] * 3, axis=3)\n",
        "        attB = np.concatenate([attB] * 3, axis=3)\n",
        "        attA_fake = np.concatenate([attA_fake] * 3, axis=3)\n",
        "        attB_fake = np.concatenate([attB_fake] * 3, axis=3)\n",
        "        \n",
        "#         print('imgA ' +  str(np.amax(imgA)) + \" - \" + str(np.amin(imgA)))\n",
        "#         print('attA ' + str(np.amax(attA)) + \" - \" + str(np.amin(attA)))\n",
        "#         print('fakeB ' + str(np.amax(fakeB)) + \" - \" + str(np.amin(fakeB)))\n",
        "#         print('cyclicA ' + str(np.amax(cyclicA)) + \" - \" + str(np.amin(cyclicA)))\n",
        "#         print('imgB ' + str(np.amax(imgB)) + \" - \" + str(np.amin(imgB)))\n",
        "#         print('attB ' + str(np.amax(attB)) + \" - \" + str(np.amin(attB)))\n",
        "#         print('fakeA ' + str(np.amax(fakeA)) + \" - \" + str(np.amin(fakeA)))\n",
        "#         print('cyclicB ' + str(np.amax(cyclicB)) + \" - \" + str(np.amin(cyclicB)))\n",
        "\n",
        "\n",
        "        gen_imgs = np.concatenate([imgA, attA, imgA_fg, fakeB_fg, fakeB, attB_fake, fakeB_fg, cyclicA_fg, cyclicA,\n",
        "                                   imgB, attB, imgB_fg, fakeA_fg, fakeA, attA_fake, fakeA_fg, cyclicB_fg, cyclicB])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Attention', 'Original fg', 'Translated fg', 'Translated', 'Attention', 'Fake fg', 'Cyclic fg', 'Cyclic']\n",
        "        fig, axs = plt.subplots(r, c, figsize=(64, 64))\n",
        "        \n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i, j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(\"output/{}/att{:0>3d}_{:d}.png\".format(self.data.name, epoch, batch_i // 1000))\n",
        "        plt.close()\n",
        "        plt.clf() \n",
        "        \n",
        "        \n",
        "    def save_things_to_drive(self, epoch, it):\n",
        "        mod_name = 'full_orig'\n",
        "\n",
        "        file_paths = ['model/{}/{}/combined1_{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it),\n",
        "                     'model/{}/{}/combined2_{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it),\n",
        "                     'model/{}/{}/disA{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it),\n",
        "                     'model/{}/{}/disB{:03d}_{:d}.h5'.format(self.data.name, mod_name, epoch, it),\n",
        "                     \"output/{}/att{:0>3d}_{:d}.png\".format(self.data.name, epoch, it)]\n",
        "            \n",
        "        name = 'attGAN_orig_' + str(epoch) + '_' + str(it) + '.zip'\n",
        "        with ZipFile(name,'w') as zip: \n",
        "            for file in file_paths: \n",
        "                zip.write(file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDaXQ5WqvkA1",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlAx6OAqvmQ4",
        "colab_type": "code",
        "outputId": "9ea93b9a-3476-46e0-8ae7-6c2ac2f7f17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11646
        }
      },
      "source": [
        "import os\n",
        "# import argparse\n",
        "\n",
        "class Dummy():\n",
        "    pass\n",
        "\n",
        "args = Dummy()\n",
        "args.dataset = 'sim2larvae'\n",
        "# args.datasets_dir = 'datasets'\n",
        "args.img_shape = 256\n",
        "args.patch = None\n",
        "args.batch_size = 1\n",
        "args.epochs = 33\n",
        "args.epoch_decay = 10\n",
        "args.lr_dis = 0.0002\n",
        "args.lr_gen = 0.0002\n",
        "args.beta1 = 0.5\n",
        "args.att = True\n",
        "args.att_epochs = 10\n",
        "args.tau = 0.1\n",
        "args.pool_size = 50\n",
        "args.sample_int = 1\n",
        "\n",
        "# output_dir\n",
        "if not os.path.exists('./output'):\n",
        "    os.makedirs('output')\n",
        "\n",
        "output_dir = './output/' + args.dataset\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# # save settings\n",
        "# with open(output_dir + '/args.yaml', 'w') as f:\n",
        "#     yaml.dump(args, f)\n",
        "\n",
        "data = Data_Loader(img_shape=(args.img_shape, args.img_shape, 3),\n",
        "                                  name=args.dataset,\n",
        "                                  patch=args.patch,\n",
        "                                  batch_size=args.batch_size)\n",
        "print(\"setting up...\")\n",
        "model = Model2(data_loader=data,\n",
        "                    epochs=args.epochs,\n",
        "                    lr_d=args.lr_dis,\n",
        "                    lr_g=args.lr_gen,\n",
        "                    beta1=args.beta1,\n",
        "                    epoch_decay=args.epoch_decay,\n",
        "                    pool_size=args.pool_size,\n",
        "                    load_from_checkpoint=False,\n",
        "                    use_att=args.att,\n",
        "                    tau=args.tau,\n",
        "                    attention_epochs_threshold=args.att_epochs)\n",
        "\n",
        "print(\"training...\")\n",
        "model.train(sample_interval=args.sample_int)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setting up...\n",
            "training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0516 13:41:58.077790 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:00.885532 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:03.075507 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:03.631710 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:04.203082 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:04.231046 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 13:42:04.276283 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/33] [Batch 50/3147] [D loss: 1.845066] [G loss: 5.269507] time: 0:02:12.526062 \n",
            "[Epoch 1/33] [Batch 100/3147] [D loss: 1.683162] [G loss: 5.079106] time: 0:02:53.456823 \n",
            "[Epoch 1/33] [Batch 150/3147] [D loss: 1.722795] [G loss: 2.832256] time: 0:03:34.691788 \n",
            "[Epoch 1/33] [Batch 200/3147] [D loss: 2.590884] [G loss: 4.405252] time: 0:04:16.028228 \n",
            "[Epoch 1/33] [Batch 250/3147] [D loss: 3.155084] [G loss: 3.089338] time: 0:04:57.251742 \n",
            "[Epoch 1/33] [Batch 300/3147] [D loss: 2.612699] [G loss: 3.393857] time: 0:05:38.705265 \n",
            "[Epoch 1/33] [Batch 350/3147] [D loss: 1.901394] [G loss: 3.070861] time: 0:06:20.023468 \n",
            "[Epoch 1/33] [Batch 400/3147] [D loss: 2.255704] [G loss: 2.616856] time: 0:07:01.188499 \n",
            "[Epoch 1/33] [Batch 450/3147] [D loss: 2.251228] [G loss: 1.911139] time: 0:07:42.874667 \n",
            "[Epoch 1/33] [Batch 500/3147] [D loss: 2.950078] [G loss: 3.515720] time: 0:08:24.107202 \n",
            "[Epoch 1/33] [Batch 550/3147] [D loss: 3.222823] [G loss: 2.815748] time: 0:09:05.823141 \n",
            "[Epoch 1/33] [Batch 600/3147] [D loss: 3.592646] [G loss: 4.544909] time: 0:09:47.132980 \n",
            "[Epoch 1/33] [Batch 650/3147] [D loss: 2.561112] [G loss: 3.676077] time: 0:10:29.141431 \n",
            "[Epoch 1/33] [Batch 700/3147] [D loss: 2.671523] [G loss: 4.956252] time: 0:11:10.474661 \n",
            "[Epoch 1/33] [Batch 750/3147] [D loss: 3.702095] [G loss: 4.743500] time: 0:11:51.873902 \n",
            "[Epoch 1/33] [Batch 800/3147] [D loss: 3.820006] [G loss: 4.390621] time: 0:12:33.194987 \n",
            "[Epoch 1/33] [Batch 850/3147] [D loss: 2.312384] [G loss: 5.246020] time: 0:13:14.627408 \n",
            "[Epoch 1/33] [Batch 900/3147] [D loss: 2.408434] [G loss: 5.337896] time: 0:13:55.985473 \n",
            "[Epoch 1/33] [Batch 950/3147] [D loss: 2.026432] [G loss: 4.144605] time: 0:14:37.577699 \n",
            "[Epoch 1/33] [Batch 1000/3147] [D loss: 2.963440] [G loss: 5.207577] time: 0:15:18.927846 \n",
            "[Epoch 1/33] [Batch 1050/3147] [D loss: 4.893990] [G loss: 5.484821] time: 0:16:06.489346 \n",
            "[Epoch 1/33] [Batch 1100/3147] [D loss: 3.603704] [G loss: 3.807066] time: 0:16:47.889558 \n",
            "[Epoch 1/33] [Batch 1150/3147] [D loss: 5.424492] [G loss: 8.551525] time: 0:17:28.826758 \n",
            "[Epoch 1/33] [Batch 1200/3147] [D loss: 4.910924] [G loss: 6.495283] time: 0:18:10.096128 \n",
            "[Epoch 1/33] [Batch 1250/3147] [D loss: 2.849544] [G loss: 6.225369] time: 0:18:50.968799 \n",
            "[Epoch 1/33] [Batch 1300/3147] [D loss: 2.459918] [G loss: 5.249873] time: 0:19:32.286411 \n",
            "[Epoch 1/33] [Batch 1350/3147] [D loss: 3.187854] [G loss: 3.987062] time: 0:20:13.184146 \n",
            "[Epoch 1/33] [Batch 1400/3147] [D loss: 2.507350] [G loss: 5.761203] time: 0:20:54.596886 \n",
            "[Epoch 1/33] [Batch 1450/3147] [D loss: 2.631426] [G loss: 5.227076] time: 0:21:35.510706 \n",
            "[Epoch 1/33] [Batch 1500/3147] [D loss: 2.809979] [G loss: 5.585290] time: 0:22:16.746980 \n",
            "[Epoch 1/33] [Batch 1550/3147] [D loss: 3.171395] [G loss: 5.188574] time: 0:22:57.777668 \n",
            "[Epoch 1/33] [Batch 1600/3147] [D loss: 2.992015] [G loss: 4.379189] time: 0:23:39.045579 \n",
            "[Epoch 1/33] [Batch 1650/3147] [D loss: 2.952398] [G loss: 4.537117] time: 0:24:20.127067 \n",
            "[Epoch 1/33] [Batch 1700/3147] [D loss: 2.152016] [G loss: 4.275726] time: 0:25:01.249267 \n",
            "[Epoch 1/33] [Batch 1750/3147] [D loss: 2.769257] [G loss: 4.644067] time: 0:25:42.191674 \n",
            "[Epoch 1/33] [Batch 1800/3147] [D loss: 2.445801] [G loss: 3.583659] time: 0:26:23.713923 \n",
            "[Epoch 1/33] [Batch 1850/3147] [D loss: 2.866382] [G loss: 2.993294] time: 0:27:04.954963 \n",
            "[Epoch 1/33] [Batch 1900/3147] [D loss: 2.190683] [G loss: 3.040911] time: 0:27:46.267023 \n",
            "[Epoch 1/33] [Batch 1950/3147] [D loss: 2.327845] [G loss: 3.441758] time: 0:28:27.434863 \n",
            "[Epoch 1/33] [Batch 2000/3147] [D loss: 2.345012] [G loss: 3.384508] time: 0:29:08.764421 \n",
            "[Epoch 1/33] [Batch 2050/3147] [D loss: 2.164331] [G loss: 3.602161] time: 0:29:55.492984 \n",
            "[Epoch 1/33] [Batch 2100/3147] [D loss: 2.412196] [G loss: 3.048238] time: 0:30:36.762327 \n",
            "[Epoch 1/33] [Batch 2150/3147] [D loss: 2.495655] [G loss: 2.856614] time: 0:31:18.192357 \n",
            "[Epoch 1/33] [Batch 2200/3147] [D loss: 2.510680] [G loss: 3.185700] time: 0:31:59.528415 \n",
            "[Epoch 1/33] [Batch 2250/3147] [D loss: 1585.151611] [G loss: 133.607056] time: 0:32:40.497875 \n",
            "[Epoch 1/33] [Batch 2300/3147] [D loss: 2.353189] [G loss: 3.109830] time: 0:33:21.808250 \n",
            "[Epoch 1/33] [Batch 2350/3147] [D loss: 3.168624] [G loss: 2.497965] time: 0:34:02.779591 \n",
            "[Epoch 1/33] [Batch 2400/3147] [D loss: 2.261659] [G loss: 3.377509] time: 0:34:43.982415 \n",
            "[Epoch 1/33] [Batch 2450/3147] [D loss: 1.939809] [G loss: 2.844158] time: 0:35:24.926092 \n",
            "[Epoch 1/33] [Batch 2500/3147] [D loss: 2.460853] [G loss: 2.995249] time: 0:36:06.133376 \n",
            "[Epoch 1/33] [Batch 2550/3147] [D loss: 2.193692] [G loss: 3.126121] time: 0:36:47.749696 \n",
            "[Epoch 1/33] [Batch 2600/3147] [D loss: 2.180968] [G loss: 2.574426] time: 0:37:28.805599 \n",
            "[Epoch 1/33] [Batch 2650/3147] [D loss: 2.467884] [G loss: 3.075320] time: 0:38:10.015135 \n",
            "[Epoch 1/33] [Batch 2700/3147] [D loss: 2.529415] [G loss: 3.078063] time: 0:38:51.127531 \n",
            "[Epoch 1/33] [Batch 2750/3147] [D loss: 2.410071] [G loss: 2.807510] time: 0:39:32.316929 \n",
            "[Epoch 1/33] [Batch 2800/3147] [D loss: 3.028782] [G loss: 3.204402] time: 0:40:13.434006 \n",
            "[Epoch 1/33] [Batch 2850/3147] [D loss: 2.530069] [G loss: 5.141062] time: 0:40:54.579822 \n",
            "[Epoch 1/33] [Batch 2900/3147] [D loss: 2.657583] [G loss: 4.636736] time: 0:41:35.849089 \n",
            "[Epoch 1/33] [Batch 2950/3147] [D loss: 2.564849] [G loss: 3.700941] time: 0:42:16.939067 \n",
            "[Epoch 1/33] [Batch 3000/3147] [D loss: 2.307352] [G loss: 3.570602] time: 0:42:57.850953 \n",
            "[Epoch 1/33] [Batch 3050/3147] [D loss: 1.992990] [G loss: 3.154377] time: 0:43:45.063649 \n",
            "[Epoch 1/33] [Batch 3100/3147] [D loss: 2.477273] [G loss: 3.744309] time: 0:44:25.797629 \n",
            "[Epoch 2/33] [Batch 50/3147] [D loss: 2.335357] [G loss: 3.496496] time: 0:45:45.461724 \n",
            "[Epoch 2/33] [Batch 100/3147] [D loss: 2.443019] [G loss: 3.726472] time: 0:46:26.461240 \n",
            "[Epoch 2/33] [Batch 150/3147] [D loss: 2.660378] [G loss: 3.658423] time: 0:47:07.679670 \n",
            "[Epoch 2/33] [Batch 200/3147] [D loss: 2.672256] [G loss: 3.801972] time: 0:47:48.716253 \n",
            "[Epoch 2/33] [Batch 250/3147] [D loss: 2.447380] [G loss: 3.563729] time: 0:48:29.658662 \n",
            "[Epoch 2/33] [Batch 300/3147] [D loss: 2.931320] [G loss: 3.938658] time: 0:49:10.811224 \n",
            "[Epoch 2/33] [Batch 350/3147] [D loss: 2.426753] [G loss: 3.672983] time: 0:49:51.710988 \n",
            "[Epoch 2/33] [Batch 400/3147] [D loss: 3.051322] [G loss: 4.023658] time: 0:50:32.908346 \n",
            "[Epoch 2/33] [Batch 450/3147] [D loss: 2.642197] [G loss: 3.847980] time: 0:51:13.755348 \n",
            "[Epoch 2/33] [Batch 500/3147] [D loss: 2.829483] [G loss: 3.892782] time: 0:51:55.187193 \n",
            "[Epoch 2/33] [Batch 550/3147] [D loss: 2.879583] [G loss: 4.074170] time: 0:52:36.059437 \n",
            "[Epoch 2/33] [Batch 600/3147] [D loss: 3.308148] [G loss: 4.352326] time: 0:53:17.245121 \n",
            "[Epoch 2/33] [Batch 650/3147] [D loss: 2.823059] [G loss: 4.310099] time: 0:53:58.003088 \n",
            "[Epoch 2/33] [Batch 700/3147] [D loss: 2.728271] [G loss: 4.379145] time: 0:54:39.257767 \n",
            "[Epoch 2/33] [Batch 750/3147] [D loss: 2.847204] [G loss: 3.955034] time: 0:55:20.147628 \n",
            "[Epoch 2/33] [Batch 800/3147] [D loss: 3.245664] [G loss: 4.112438] time: 0:56:01.362115 \n",
            "[Epoch 2/33] [Batch 850/3147] [D loss: 3.160729] [G loss: 4.370637] time: 0:56:42.313241 \n",
            "[Epoch 2/33] [Batch 900/3147] [D loss: 2.990777] [G loss: 4.032372] time: 0:57:23.834194 \n",
            "[Epoch 2/33] [Batch 950/3147] [D loss: 2.587435] [G loss: 4.426923] time: 0:58:04.563080 \n",
            "[Epoch 2/33] [Batch 1000/3147] [D loss: 2.624298] [G loss: 3.895074] time: 0:58:45.688573 \n",
            "[Epoch 2/33] [Batch 1050/3147] [D loss: 2.113560] [G loss: 3.399609] time: 0:59:31.742475 \n",
            "[Epoch 2/33] [Batch 1100/3147] [D loss: 2.876631] [G loss: 3.900252] time: 1:00:12.886329 \n",
            "[Epoch 2/33] [Batch 1150/3147] [D loss: 3.247755] [G loss: 4.131715] time: 1:00:53.728209 \n",
            "[Epoch 2/33] [Batch 1200/3147] [D loss: 2.717979] [G loss: 3.885834] time: 1:01:34.778400 \n",
            "[Epoch 2/33] [Batch 1250/3147] [D loss: 2.880360] [G loss: 3.963640] time: 1:02:15.920189 \n",
            "[Epoch 2/33] [Batch 1300/3147] [D loss: 2.723140] [G loss: 3.902137] time: 1:02:56.790841 \n",
            "[Epoch 2/33] [Batch 1350/3147] [D loss: 2.826807] [G loss: 3.887907] time: 1:03:37.680200 \n",
            "[Epoch 2/33] [Batch 1400/3147] [D loss: 3.006557] [G loss: 4.069584] time: 1:04:18.616209 \n",
            "[Epoch 2/33] [Batch 1450/3147] [D loss: 166.540298] [G loss: 10.564841] time: 1:04:59.771187 \n",
            "[Epoch 2/33] [Batch 1500/3147] [D loss: 3.062896] [G loss: 4.644442] time: 1:05:40.682379 \n",
            "[Epoch 2/33] [Batch 1550/3147] [D loss: 3.248038] [G loss: 3.801704] time: 1:06:21.816026 \n",
            "[Epoch 2/33] [Batch 1600/3147] [D loss: 2.955786] [G loss: 3.963533] time: 1:07:02.695949 \n",
            "[Epoch 2/33] [Batch 1650/3147] [D loss: 3.000729] [G loss: 3.893223] time: 1:07:44.273524 \n",
            "[Epoch 2/33] [Batch 1700/3147] [D loss: 2.958375] [G loss: 3.867344] time: 1:08:25.138330 \n",
            "[Epoch 2/33] [Batch 1750/3147] [D loss: 2.687800] [G loss: 3.869426] time: 1:09:06.338438 \n",
            "[Epoch 2/33] [Batch 1800/3147] [D loss: 3.019359] [G loss: 4.075779] time: 1:09:47.116181 \n",
            "[Epoch 2/33] [Batch 1850/3147] [D loss: 3.318253] [G loss: 4.149145] time: 1:10:28.220984 \n",
            "[Epoch 2/33] [Batch 1900/3147] [D loss: 3.645770] [G loss: 4.183119] time: 1:11:08.992190 \n",
            "[Epoch 2/33] [Batch 1950/3147] [D loss: 3.677147] [G loss: 4.231464] time: 1:11:49.986672 \n",
            "[Epoch 2/33] [Batch 2000/3147] [D loss: 3.175625] [G loss: 4.042844] time: 1:12:30.834058 \n",
            "[Epoch 2/33] [Batch 2050/3147] [D loss: 3.895685] [G loss: 3.990852] time: 1:13:17.983517 \n",
            "[Epoch 2/33] [Batch 2100/3147] [D loss: 3.145948] [G loss: 4.410131] time: 1:13:58.714201 \n",
            "[Epoch 2/33] [Batch 2150/3147] [D loss: 3.141947] [G loss: 3.953764] time: 1:14:39.798828 \n",
            "[Epoch 2/33] [Batch 2200/3147] [D loss: 2.810231] [G loss: 3.798217] time: 1:15:20.820310 \n",
            "[Epoch 2/33] [Batch 2250/3147] [D loss: 2.609204] [G loss: 4.352631] time: 1:16:01.806548 \n",
            "[Epoch 2/33] [Batch 2300/3147] [D loss: 269.349426] [G loss: 21.319365] time: 1:16:42.572492 \n",
            "[Epoch 2/33] [Batch 2350/3147] [D loss: 654.683167] [G loss: 27.144613] time: 1:17:23.713774 \n",
            "[Epoch 2/33] [Batch 2400/3147] [D loss: 339.566040] [G loss: 26.268122] time: 1:18:05.001806 \n",
            "[Epoch 2/33] [Batch 2450/3147] [D loss: 109.918007] [G loss: 20.732573] time: 1:18:46.299540 \n",
            "[Epoch 2/33] [Batch 2500/3147] [D loss: 95.157043] [G loss: 20.474926] time: 1:19:27.114736 \n",
            "[Epoch 2/33] [Batch 2550/3147] [D loss: 501.136230] [G loss: 31.744864] time: 1:20:08.383065 \n",
            "[Epoch 2/33] [Batch 2600/3147] [D loss: 241.872986] [G loss: 39.063526] time: 1:20:49.244343 \n",
            "[Epoch 2/33] [Batch 2650/3147] [D loss: 192.816803] [G loss: 24.942783] time: 1:21:30.425951 \n",
            "[Epoch 2/33] [Batch 2700/3147] [D loss: 70.307076] [G loss: 20.246450] time: 1:22:11.246510 \n",
            "[Epoch 2/33] [Batch 2750/3147] [D loss: 95.885246] [G loss: 22.188559] time: 1:22:52.584102 \n",
            "[Epoch 2/33] [Batch 2800/3147] [D loss: 19.334282] [G loss: 9.367951] time: 1:23:33.606717 \n",
            "[Epoch 2/33] [Batch 2850/3147] [D loss: 55.520485] [G loss: 19.835287] time: 1:24:14.769728 \n",
            "[Epoch 2/33] [Batch 2900/3147] [D loss: 61.095078] [G loss: 20.629417] time: 1:24:55.673687 \n",
            "[Epoch 2/33] [Batch 2950/3147] [D loss: 86.951645] [G loss: 22.438894] time: 1:25:36.942934 \n",
            "[Epoch 2/33] [Batch 3000/3147] [D loss: 56.407017] [G loss: 16.647385] time: 1:26:17.892992 \n",
            "[Epoch 2/33] [Batch 3050/3147] [D loss: 20.046402] [G loss: 11.428871] time: 1:27:04.797715 \n",
            "[Epoch 2/33] [Batch 3100/3147] [D loss: 73.360710] [G loss: 17.002470] time: 1:27:45.767828 \n",
            "[Epoch 3/33] [Batch 50/3147] [D loss: 22.647316] [G loss: 10.032195] time: 1:29:05.517415 \n",
            "[Epoch 3/33] [Batch 100/3147] [D loss: 46.049011] [G loss: 13.576760] time: 1:29:46.442909 \n",
            "[Epoch 3/33] [Batch 150/3147] [D loss: 62.372066] [G loss: 14.321933] time: 1:30:27.553886 \n",
            "[Epoch 3/33] [Batch 200/3147] [D loss: 31.233715] [G loss: 9.905357] time: 1:31:08.462070 \n",
            "[Epoch 3/33] [Batch 250/3147] [D loss: 19.161798] [G loss: 9.245131] time: 1:31:49.472312 \n",
            "[Epoch 3/33] [Batch 300/3147] [D loss: 7.375568] [G loss: 6.136734] time: 1:32:30.323531 \n",
            "[Epoch 3/33] [Batch 350/3147] [D loss: 12.934890] [G loss: 7.711904] time: 1:33:11.415822 \n",
            "[Epoch 3/33] [Batch 400/3147] [D loss: 4.650598] [G loss: 5.971302] time: 1:33:52.394318 \n",
            "[Epoch 3/33] [Batch 450/3147] [D loss: 6.183565] [G loss: 6.474693] time: 1:34:33.517805 \n",
            "[Epoch 3/33] [Batch 500/3147] [D loss: 9.222851] [G loss: 8.297883] time: 1:35:14.215352 \n",
            "[Epoch 3/33] [Batch 550/3147] [D loss: 2.360620] [G loss: 2.924690] time: 1:35:55.463154 \n",
            "[Epoch 3/33] [Batch 600/3147] [D loss: 2.244647] [G loss: 4.921091] time: 1:36:36.184210 \n",
            "[Epoch 3/33] [Batch 650/3147] [D loss: 10.617693] [G loss: 6.197605] time: 1:37:17.341079 \n",
            "[Epoch 3/33] [Batch 700/3147] [D loss: 8.019221] [G loss: 5.349600] time: 1:37:58.087272 \n",
            "[Epoch 3/33] [Batch 750/3147] [D loss: 3.737745] [G loss: 4.152936] time: 1:38:39.211095 \n",
            "[Epoch 3/33] [Batch 800/3147] [D loss: 4.854032] [G loss: 5.048801] time: 1:39:19.967680 \n",
            "[Epoch 3/33] [Batch 850/3147] [D loss: 3.495220] [G loss: 4.175998] time: 1:40:00.980442 \n",
            "[Epoch 3/33] [Batch 900/3147] [D loss: 3.124132] [G loss: 4.461473] time: 1:40:41.964797 \n",
            "[Epoch 3/33] [Batch 950/3147] [D loss: 2.916939] [G loss: 4.481019] time: 1:41:23.039579 \n",
            "[Epoch 3/33] [Batch 1000/3147] [D loss: 3.489667] [G loss: 3.980070] time: 1:42:03.732168 \n",
            "[Epoch 3/33] [Batch 1050/3147] [D loss: 2.332512] [G loss: 3.134938] time: 1:42:49.840810 \n",
            "[Epoch 3/33] [Batch 1100/3147] [D loss: 2.357516] [G loss: 3.407908] time: 1:43:30.572863 \n",
            "[Epoch 3/33] [Batch 1150/3147] [D loss: 2.437999] [G loss: 2.921454] time: 1:44:11.906413 \n",
            "[Epoch 3/33] [Batch 1200/3147] [D loss: 2.215011] [G loss: 2.778945] time: 1:44:52.706698 \n",
            "[Epoch 3/33] [Batch 1250/3147] [D loss: 2.156911] [G loss: 2.443280] time: 1:45:33.652251 \n",
            "[Epoch 3/33] [Batch 1300/3147] [D loss: 2.115695] [G loss: 3.485029] time: 1:46:14.462923 \n",
            "[Epoch 3/33] [Batch 1350/3147] [D loss: 2.159659] [G loss: 2.576165] time: 1:46:55.448143 \n",
            "[Epoch 3/33] [Batch 1400/3147] [D loss: 2.285088] [G loss: 2.570933] time: 1:47:36.237384 \n",
            "[Epoch 3/33] [Batch 1450/3147] [D loss: 2.361058] [G loss: 2.481127] time: 1:48:17.340988 \n",
            "[Epoch 3/33] [Batch 1500/3147] [D loss: 2.059775] [G loss: 2.782141] time: 1:48:58.455281 \n",
            "[Epoch 3/33] [Batch 1550/3147] [D loss: 2.266784] [G loss: 2.495995] time: 1:49:39.520264 \n",
            "[Epoch 3/33] [Batch 1600/3147] [D loss: 2.334383] [G loss: 2.819901] time: 1:50:20.248132 \n",
            "[Epoch 3/33] [Batch 1650/3147] [D loss: 1.914916] [G loss: 2.603663] time: 1:51:01.421208 \n",
            "[Epoch 3/33] [Batch 1700/3147] [D loss: 143750608.000000] [G loss: 39861.832031] time: 1:51:42.131957 \n",
            "[Epoch 3/33] [Batch 1750/3147] [D loss: 117.880524] [G loss: 10.611132] time: 1:52:23.115382 \n",
            "[Epoch 3/33] [Batch 1800/3147] [D loss: 32.219231] [G loss: 7.311976] time: 1:53:03.654108 \n",
            "[Epoch 3/33] [Batch 1850/3147] [D loss: 23.419725] [G loss: 6.459636] time: 1:53:44.629138 \n",
            "[Epoch 3/33] [Batch 1900/3147] [D loss: 28.073780] [G loss: 6.930382] time: 1:54:25.753640 \n",
            "[Epoch 3/33] [Batch 1950/3147] [D loss: 33.011406] [G loss: 7.563475] time: 1:55:06.670883 \n",
            "[Epoch 3/33] [Batch 2000/3147] [D loss: 492.292572] [G loss: 25.151100] time: 1:55:47.408322 \n",
            "[Epoch 3/33] [Batch 2050/3147] [D loss: 87.048584] [G loss: 12.753098] time: 1:56:34.069740 \n",
            "[Epoch 3/33] [Batch 2100/3147] [D loss: 107.225563] [G loss: 12.669325] time: 1:57:14.945438 \n",
            "[Epoch 3/33] [Batch 2150/3147] [D loss: 120.114822] [G loss: 12.541633] time: 1:57:55.570004 \n",
            "[Epoch 3/33] [Batch 2200/3147] [D loss: 131.512558] [G loss: 13.290564] time: 1:58:36.436483 \n",
            "[Epoch 3/33] [Batch 2250/3147] [D loss: 130.621262] [G loss: 13.192781] time: 1:59:17.532249 \n",
            "[Epoch 3/33] [Batch 2300/3147] [D loss: 143.509125] [G loss: 13.162930] time: 1:59:58.446944 \n",
            "[Epoch 3/33] [Batch 2350/3147] [D loss: 98.115784] [G loss: 11.864340] time: 2:00:39.120633 \n",
            "[Epoch 3/33] [Batch 2400/3147] [D loss: 73.294937] [G loss: 11.577807] time: 2:01:20.215549 \n",
            "[Epoch 3/33] [Batch 2450/3147] [D loss: 57.927357] [G loss: 11.030385] time: 2:02:00.817083 \n",
            "[Epoch 3/33] [Batch 2500/3147] [D loss: 42.970234] [G loss: 10.634222] time: 2:02:41.782011 \n",
            "[Epoch 3/33] [Batch 2550/3147] [D loss: 37.290726] [G loss: 10.507551] time: 2:03:22.467603 \n",
            "[Epoch 3/33] [Batch 2600/3147] [D loss: 36.625141] [G loss: 10.130293] time: 2:04:03.364009 \n",
            "[Epoch 3/33] [Batch 2650/3147] [D loss: 23.194145] [G loss: 8.937804] time: 2:04:44.420320 \n",
            "[Epoch 3/33] [Batch 2700/3147] [D loss: 23.722347] [G loss: 8.133488] time: 2:05:25.312416 \n",
            "[Epoch 3/33] [Batch 2750/3147] [D loss: 28.455425] [G loss: 8.462171] time: 2:06:06.074101 \n",
            "[Epoch 3/33] [Batch 2800/3147] [D loss: 10.638633] [G loss: 7.950103] time: 2:06:47.007852 \n",
            "[Epoch 3/33] [Batch 2850/3147] [D loss: 10.816305] [G loss: 8.287195] time: 2:07:27.620185 \n",
            "[Epoch 3/33] [Batch 2900/3147] [D loss: 10.361916] [G loss: 8.206765] time: 2:08:08.494127 \n",
            "[Epoch 3/33] [Batch 2950/3147] [D loss: 8.195879] [G loss: 7.961003] time: 2:08:49.054127 \n",
            "[Epoch 3/33] [Batch 3000/3147] [D loss: 18.341877] [G loss: 6.645158] time: 2:09:30.136574 \n",
            "[Epoch 3/33] [Batch 3050/3147] [D loss: 10.070354] [G loss: 6.988063] time: 2:10:16.181722 \n",
            "[Epoch 3/33] [Batch 3100/3147] [D loss: 9.367086] [G loss: 6.776499] time: 2:10:56.958554 \n",
            "[Epoch 4/33] [Batch 50/3147] [D loss: 6.411819] [G loss: 5.579547] time: 2:12:16.181790 \n",
            "[Epoch 4/33] [Batch 100/3147] [D loss: 2.940986] [G loss: 5.375997] time: 2:12:56.745008 \n",
            "[Epoch 4/33] [Batch 150/3147] [D loss: 5.929963] [G loss: 5.429715] time: 2:13:37.647725 \n",
            "[Epoch 4/33] [Batch 200/3147] [D loss: 10.826670] [G loss: 4.696026] time: 2:14:18.229400 \n",
            "[Epoch 4/33] [Batch 250/3147] [D loss: 2.834623] [G loss: 4.441728] time: 2:14:59.252891 \n",
            "[Epoch 4/33] [Batch 300/3147] [D loss: 3.572472] [G loss: 3.738421] time: 2:15:39.802465 \n",
            "[Epoch 4/33] [Batch 350/3147] [D loss: 3.246741] [G loss: 4.262046] time: 2:16:20.752876 \n",
            "[Epoch 4/33] [Batch 400/3147] [D loss: 2.633498] [G loss: 3.594263] time: 2:17:01.166578 \n",
            "[Epoch 4/33] [Batch 450/3147] [D loss: 2.548510] [G loss: 4.101785] time: 2:17:41.951113 \n",
            "[Epoch 4/33] [Batch 500/3147] [D loss: 3.398101] [G loss: 2.918696] time: 2:18:22.560814 \n",
            "[Epoch 4/33] [Batch 550/3147] [D loss: 2.171807] [G loss: 3.530228] time: 2:19:03.318360 \n",
            "[Epoch 4/33] [Batch 600/3147] [D loss: 4.818003] [G loss: 2.374520] time: 2:19:43.881337 \n",
            "[Epoch 4/33] [Batch 650/3147] [D loss: 2.043074] [G loss: 2.918797] time: 2:20:25.050749 \n",
            "[Epoch 4/33] [Batch 700/3147] [D loss: 1.990182] [G loss: 2.994956] time: 2:21:05.746260 \n",
            "[Epoch 4/33] [Batch 750/3147] [D loss: 1.811794] [G loss: 3.155458] time: 2:21:46.842313 \n",
            "[Epoch 4/33] [Batch 800/3147] [D loss: 1.705808] [G loss: 2.634088] time: 2:22:27.496927 \n",
            "[Epoch 4/33] [Batch 850/3147] [D loss: 1.744772] [G loss: 2.598061] time: 2:23:08.252268 \n",
            "[Epoch 4/33] [Batch 900/3147] [D loss: 1.534979] [G loss: 2.782295] time: 2:23:48.945086 \n",
            "[Epoch 4/33] [Batch 950/3147] [D loss: 1.763386] [G loss: 3.091299] time: 2:24:29.717529 \n",
            "[Epoch 4/33] [Batch 1000/3147] [D loss: 2.224007] [G loss: 3.134143] time: 2:25:10.674814 \n",
            "[Epoch 4/33] [Batch 1050/3147] [D loss: 1.626467] [G loss: 2.746187] time: 2:25:56.596255 \n",
            "[Epoch 4/33] [Batch 1100/3147] [D loss: 1.922986] [G loss: 2.723437] time: 2:26:37.448356 \n",
            "[Epoch 4/33] [Batch 1150/3147] [D loss: 2.501250] [G loss: 2.693623] time: 2:27:18.249341 \n",
            "[Epoch 4/33] [Batch 1200/3147] [D loss: 1.637178] [G loss: 2.938637] time: 2:27:58.837533 \n",
            "[Epoch 4/33] [Batch 1250/3147] [D loss: 2.062373] [G loss: 2.489125] time: 2:28:39.615031 \n",
            "[Epoch 4/33] [Batch 1300/3147] [D loss: 4.140720] [G loss: 2.119982] time: 2:29:20.231763 \n",
            "[Epoch 4/33] [Batch 1350/3147] [D loss: 1.710173] [G loss: 2.725714] time: 2:30:00.970377 \n",
            "[Epoch 4/33] [Batch 1400/3147] [D loss: 1.662461] [G loss: 2.517649] time: 2:30:41.975726 \n",
            "[Epoch 4/33] [Batch 1450/3147] [D loss: 1.654343] [G loss: 2.917527] time: 2:31:22.930607 \n",
            "[Epoch 4/33] [Batch 1500/3147] [D loss: 2.075255] [G loss: 2.030735] time: 2:32:03.670856 \n",
            "[Epoch 4/33] [Batch 1550/3147] [D loss: 1.782538] [G loss: 2.876759] time: 2:32:44.234369 \n",
            "[Epoch 4/33] [Batch 1600/3147] [D loss: 1.717604] [G loss: 2.374109] time: 2:33:24.885339 \n",
            "[Epoch 4/33] [Batch 1650/3147] [D loss: 1.625505] [G loss: 2.376348] time: 2:34:05.344681 \n",
            "[Epoch 4/33] [Batch 1700/3147] [D loss: 1.636199] [G loss: 2.770959] time: 2:34:45.969938 \n",
            "[Epoch 4/33] [Batch 1750/3147] [D loss: 1.533796] [G loss: 2.395513] time: 2:35:26.876178 \n",
            "[Epoch 4/33] [Batch 1800/3147] [D loss: 1.696770] [G loss: 2.761465] time: 2:36:07.381722 \n",
            "[Epoch 4/33] [Batch 1850/3147] [D loss: 433786.593750] [G loss: 447.387787] time: 2:36:47.981819 \n",
            "[Epoch 4/33] [Batch 1900/3147] [D loss: 501.550385] [G loss: 26.511122] time: 2:37:28.627129 \n",
            "[Epoch 4/33] [Batch 1950/3147] [D loss: 190.870895] [G loss: 15.269500] time: 2:38:09.106907 \n",
            "[Epoch 4/33] [Batch 2000/3147] [D loss: 71.158234] [G loss: 12.654160] time: 2:38:49.690769 \n",
            "[Epoch 4/33] [Batch 2050/3147] [D loss: 41.307182] [G loss: 7.695753] time: 2:39:35.621682 \n",
            "[Epoch 4/33] [Batch 2100/3147] [D loss: 16.286232] [G loss: 7.447092] time: 2:40:16.371143 \n",
            "[Epoch 4/33] [Batch 2150/3147] [D loss: 7.831907] [G loss: 6.851377] time: 2:40:57.117660 \n",
            "[Epoch 4/33] [Batch 2200/3147] [D loss: 10.077630] [G loss: 5.672079] time: 2:41:37.880736 \n",
            "[Epoch 4/33] [Batch 2250/3147] [D loss: 5.474018] [G loss: 5.019776] time: 2:42:18.306846 \n",
            "[Epoch 4/33] [Batch 2300/3147] [D loss: 15.250341] [G loss: 7.921771] time: 2:42:58.850198 \n",
            "[Epoch 4/33] [Batch 2350/3147] [D loss: 10.030904] [G loss: 6.217526] time: 2:43:38.971410 \n",
            "[Epoch 4/33] [Batch 2400/3147] [D loss: 12.535887] [G loss: 5.816926] time: 2:44:19.472581 \n",
            "[Epoch 4/33] [Batch 2450/3147] [D loss: 16.721830] [G loss: 6.249625] time: 2:44:59.743618 \n",
            "[Epoch 4/33] [Batch 2500/3147] [D loss: 31.461035] [G loss: 7.785256] time: 2:45:40.367899 \n",
            "[Epoch 4/33] [Batch 2550/3147] [D loss: 37.667423] [G loss: 9.051375] time: 2:46:20.707100 \n",
            "[Epoch 4/33] [Batch 2600/3147] [D loss: 67.115311] [G loss: 11.506275] time: 2:47:01.441261 \n",
            "[Epoch 4/33] [Batch 2650/3147] [D loss: 80.103340] [G loss: 12.494952] time: 2:47:41.740707 \n",
            "[Epoch 4/33] [Batch 2700/3147] [D loss: 132.377335] [G loss: 13.685330] time: 2:48:22.265246 \n",
            "[Epoch 4/33] [Batch 2750/3147] [D loss: 477.306824] [G loss: 23.306190] time: 2:49:02.547718 \n",
            "[Epoch 4/33] [Batch 2800/3147] [D loss: 568.600098] [G loss: 25.493402] time: 2:49:42.981870 \n",
            "[Epoch 4/33] [Batch 2850/3147] [D loss: 1073.269287] [G loss: 33.842495] time: 2:50:23.119534 \n",
            "[Epoch 4/33] [Batch 2900/3147] [D loss: 908.503113] [G loss: 34.401817] time: 2:51:03.882221 \n",
            "[Epoch 4/33] [Batch 2950/3147] [D loss: 6787984.500000] [G loss: 796.977539] time: 2:51:44.277962 \n",
            "[Epoch 4/33] [Batch 3000/3147] [D loss: 423.588074] [G loss: 34.028152] time: 2:52:24.829944 \n",
            "[Epoch 4/33] [Batch 3050/3147] [D loss: 1267.933716] [G loss: 48.288040] time: 2:53:09.887529 \n",
            "[Epoch 4/33] [Batch 3100/3147] [D loss: 561.816284] [G loss: 30.966848] time: 2:53:50.268512 \n",
            "[Epoch 5/33] [Batch 50/3147] [D loss: 159.401215] [G loss: 17.724987] time: 2:55:08.208027 \n",
            "[Epoch 5/33] [Batch 100/3147] [D loss: 312.339630] [G loss: 26.381899] time: 2:55:48.219482 \n",
            "[Epoch 5/33] [Batch 150/3147] [D loss: 165.245697] [G loss: 21.498255] time: 2:56:29.123893 \n",
            "[Epoch 5/33] [Batch 200/3147] [D loss: 172.170776] [G loss: 15.866031] time: 2:57:09.539436 \n",
            "[Epoch 5/33] [Batch 250/3147] [D loss: 142.437927] [G loss: 14.227416] time: 2:57:50.013116 \n",
            "[Epoch 5/33] [Batch 300/3147] [D loss: 40.616657] [G loss: 11.920221] time: 2:58:30.245869 \n",
            "[Epoch 5/33] [Batch 350/3147] [D loss: 191.297379] [G loss: 22.100777] time: 2:59:10.801029 \n",
            "[Epoch 5/33] [Batch 400/3147] [D loss: 138.691254] [G loss: 22.190744] time: 2:59:50.881301 \n",
            "[Epoch 5/33] [Batch 450/3147] [D loss: 226.537201] [G loss: 24.014225] time: 3:00:31.357072 \n",
            "[Epoch 5/33] [Batch 500/3147] [D loss: 126.122253] [G loss: 21.168653] time: 3:01:11.638606 \n",
            "[Epoch 5/33] [Batch 550/3147] [D loss: 98.428146] [G loss: 18.442427] time: 3:01:52.258687 \n",
            "[Epoch 5/33] [Batch 600/3147] [D loss: 185.314957] [G loss: 29.757132] time: 3:02:32.578461 \n",
            "[Epoch 5/33] [Batch 650/3147] [D loss: 291.697327] [G loss: 22.401236] time: 3:03:13.096263 \n",
            "[Epoch 5/33] [Batch 700/3147] [D loss: 171.651184] [G loss: 24.042917] time: 3:03:53.073843 \n",
            "[Epoch 5/33] [Batch 750/3147] [D loss: 156.071594] [G loss: 19.109945] time: 3:04:33.511046 \n",
            "[Epoch 5/33] [Batch 800/3147] [D loss: 115.873276] [G loss: 17.102219] time: 3:05:13.529687 \n",
            "[Epoch 5/33] [Batch 850/3147] [D loss: 48.942638] [G loss: 16.888996] time: 3:05:53.948311 \n",
            "[Epoch 5/33] [Batch 900/3147] [D loss: 36.944576] [G loss: 14.987318] time: 3:06:34.479760 \n",
            "[Epoch 5/33] [Batch 950/3147] [D loss: 55.998970] [G loss: 17.031944] time: 3:07:15.042022 \n",
            "[Epoch 5/33] [Batch 1000/3147] [D loss: 25.933392] [G loss: 12.368399] time: 3:07:55.095894 \n",
            "[Epoch 5/33] [Batch 1050/3147] [D loss: 56.377579] [G loss: 11.327725] time: 3:08:40.426786 \n",
            "[Epoch 5/33] [Batch 1100/3147] [D loss: 21.190069] [G loss: 10.793776] time: 3:09:20.518394 \n",
            "[Epoch 5/33] [Batch 1150/3147] [D loss: 14.884378] [G loss: 9.808317] time: 3:10:00.923394 \n",
            "[Epoch 5/33] [Batch 1200/3147] [D loss: 17.805029] [G loss: 10.591169] time: 3:10:41.066111 \n",
            "[Epoch 5/33] [Batch 1250/3147] [D loss: 17.781958] [G loss: 7.691673] time: 3:11:21.493905 \n",
            "[Epoch 5/33] [Batch 1300/3147] [D loss: 12.740137] [G loss: 8.240329] time: 3:12:02.031398 \n",
            "[Epoch 5/33] [Batch 1350/3147] [D loss: 36.325531] [G loss: 8.114760] time: 3:12:42.624668 \n",
            "[Epoch 5/33] [Batch 1400/3147] [D loss: 40.522087] [G loss: 15.069054] time: 3:13:22.656826 \n",
            "[Epoch 5/33] [Batch 1450/3147] [D loss: 8.947389] [G loss: 7.514563] time: 3:14:03.181245 \n",
            "[Epoch 5/33] [Batch 1500/3147] [D loss: 14.150447] [G loss: 7.599727] time: 3:14:43.143243 \n",
            "[Epoch 5/33] [Batch 1550/3147] [D loss: 12.821827] [G loss: 7.651229] time: 3:15:23.510236 \n",
            "[Epoch 5/33] [Batch 1600/3147] [D loss: 7.379498] [G loss: 6.991395] time: 3:16:03.694323 \n",
            "[Epoch 5/33] [Batch 1650/3147] [D loss: 5.603600] [G loss: 6.316533] time: 3:16:44.427262 \n",
            "[Epoch 5/33] [Batch 1700/3147] [D loss: 7.987187] [G loss: 7.425840] time: 3:17:24.889161 \n",
            "[Epoch 5/33] [Batch 1750/3147] [D loss: 27.276611] [G loss: 6.430377] time: 3:18:05.425213 \n",
            "[Epoch 5/33] [Batch 1800/3147] [D loss: 9.017631] [G loss: 6.629992] time: 3:18:45.634816 \n",
            "[Epoch 5/33] [Batch 1850/3147] [D loss: 6.773551] [G loss: 6.228246] time: 3:19:26.094058 \n",
            "[Epoch 5/33] [Batch 1900/3147] [D loss: 24.138634] [G loss: 7.720996] time: 3:20:06.127917 \n",
            "[Epoch 5/33] [Batch 1950/3147] [D loss: 5.741331] [G loss: 6.079795] time: 3:20:46.514413 \n",
            "[Epoch 5/33] [Batch 2000/3147] [D loss: 8.799327] [G loss: 5.661691] time: 3:21:26.574291 \n",
            "[Epoch 5/33] [Batch 2050/3147] [D loss: 19.359295] [G loss: 7.359658] time: 3:22:12.187034 \n",
            "[Epoch 5/33] [Batch 2100/3147] [D loss: 6.026135] [G loss: 5.880426] time: 3:22:52.439081 \n",
            "[Epoch 5/33] [Batch 2150/3147] [D loss: 62.099911] [G loss: 6.990487] time: 3:23:32.876607 \n",
            "[Epoch 5/33] [Batch 2200/3147] [D loss: 3.747844] [G loss: 5.353168] time: 3:24:12.990155 \n",
            "[Epoch 5/33] [Batch 2250/3147] [D loss: 5.255568] [G loss: 4.929669] time: 3:24:53.265464 \n",
            "[Epoch 5/33] [Batch 2300/3147] [D loss: 5.279884] [G loss: 4.515785] time: 3:25:33.266902 \n",
            "[Epoch 5/33] [Batch 2350/3147] [D loss: 25.974672] [G loss: 10.225891] time: 3:26:13.595993 \n",
            "[Epoch 5/33] [Batch 2400/3147] [D loss: 5.494102] [G loss: 4.683364] time: 3:26:53.596787 \n",
            "[Epoch 5/33] [Batch 2450/3147] [D loss: 3.516596] [G loss: 4.195251] time: 3:27:34.375182 \n",
            "[Epoch 5/33] [Batch 2500/3147] [D loss: 3.682345] [G loss: 5.054291] time: 3:28:14.388027 \n",
            "[Epoch 5/33] [Batch 2550/3147] [D loss: 3.981275] [G loss: 4.795588] time: 3:28:54.647648 \n",
            "[Epoch 5/33] [Batch 2600/3147] [D loss: 1.833333] [G loss: 3.242911] time: 3:29:34.553876 \n",
            "[Epoch 5/33] [Batch 2650/3147] [D loss: 2.153178] [G loss: 4.276892] time: 3:30:14.843949 \n",
            "[Epoch 5/33] [Batch 2700/3147] [D loss: 1.880075] [G loss: 3.502000] time: 3:30:54.846541 \n",
            "[Epoch 5/33] [Batch 2750/3147] [D loss: 4.947852] [G loss: 4.918219] time: 3:31:34.973320 \n",
            "[Epoch 5/33] [Batch 2800/3147] [D loss: 4081.454102] [G loss: 56.121399] time: 3:32:15.232278 \n",
            "[Epoch 5/33] [Batch 2850/3147] [D loss: 355.616913] [G loss: 23.106470] time: 3:32:55.761002 \n",
            "[Epoch 5/33] [Batch 2900/3147] [D loss: 89.692680] [G loss: 10.822372] time: 3:33:35.772869 \n",
            "[Epoch 5/33] [Batch 2950/3147] [D loss: 69.260750] [G loss: 12.108665] time: 3:34:16.097386 \n",
            "[Epoch 5/33] [Batch 3000/3147] [D loss: 53.954594] [G loss: 12.983778] time: 3:34:56.106629 \n",
            "[Epoch 5/33] [Batch 3050/3147] [D loss: 48.740723] [G loss: 10.203446] time: 3:35:41.911224 \n",
            "[Epoch 5/33] [Batch 3100/3147] [D loss: 66.831627] [G loss: 12.037236] time: 3:36:21.979593 \n",
            "[Epoch 6/33] [Batch 50/3147] [D loss: 159.874115] [G loss: 18.101412] time: 3:37:40.359004 \n",
            "[Epoch 6/33] [Batch 100/3147] [D loss: 531.903198] [G loss: 27.131458] time: 3:38:20.709367 \n",
            "[Epoch 6/33] [Batch 150/3147] [D loss: 3797.751465] [G loss: 71.533089] time: 3:39:00.737534 \n",
            "[Epoch 6/33] [Batch 200/3147] [D loss: 23271.859375] [G loss: 114.506493] time: 3:39:40.957222 \n",
            "[Epoch 6/33] [Batch 250/3147] [D loss: 31580.732422] [G loss: 133.079895] time: 3:40:21.137964 \n",
            "[Epoch 6/33] [Batch 300/3147] [D loss: 16646.404297] [G loss: 185.047470] time: 3:41:01.583847 \n",
            "[Epoch 6/33] [Batch 350/3147] [D loss: 21724.470703] [G loss: 225.433243] time: 3:41:41.859292 \n",
            "[Epoch 6/33] [Batch 400/3147] [D loss: 16134.750000] [G loss: 247.924805] time: 3:42:22.338246 \n",
            "[Epoch 6/33] [Batch 450/3147] [D loss: 13088.523438] [G loss: 145.930832] time: 3:43:03.224942 \n",
            "[Epoch 6/33] [Batch 500/3147] [D loss: 16237.112305] [G loss: 172.986938] time: 3:43:43.746214 \n",
            "[Epoch 6/33] [Batch 550/3147] [D loss: 17696.582031] [G loss: 182.300690] time: 3:44:23.978772 \n",
            "[Epoch 6/33] [Batch 600/3147] [D loss: 41781.640625] [G loss: 271.563477] time: 3:45:04.480675 \n",
            "[Epoch 6/33] [Batch 650/3147] [D loss: 21916.464844] [G loss: 222.154602] time: 3:45:44.787178 \n",
            "[Epoch 6/33] [Batch 700/3147] [D loss: 12700.040039] [G loss: 157.629227] time: 3:46:25.142914 \n",
            "[Epoch 6/33] [Batch 750/3147] [D loss: 4762.075684] [G loss: 69.928909] time: 3:47:05.450246 \n",
            "[Epoch 6/33] [Batch 800/3147] [D loss: 2757.704102] [G loss: 64.573784] time: 3:47:46.073789 \n",
            "[Epoch 6/33] [Batch 850/3147] [D loss: 4062.025391] [G loss: 65.849464] time: 3:48:26.663309 \n",
            "[Epoch 6/33] [Batch 900/3147] [D loss: 5706.996582] [G loss: 113.343018] time: 3:49:06.882832 \n",
            "[Epoch 6/33] [Batch 950/3147] [D loss: 3834.548828] [G loss: 109.945969] time: 3:49:47.185795 \n",
            "[Epoch 6/33] [Batch 1000/3147] [D loss: 3283.672852] [G loss: 99.919579] time: 3:50:27.545091 \n",
            "[Epoch 6/33] [Batch 1050/3147] [D loss: 5432.557129] [G loss: 144.811783] time: 3:51:12.745084 \n",
            "[Epoch 6/33] [Batch 1100/3147] [D loss: 573729.375000] [G loss: 640.266785] time: 3:51:52.795019 \n",
            "[Epoch 6/33] [Batch 1150/3147] [D loss: 153184.546875] [G loss: 256.686310] time: 3:52:33.095688 \n",
            "[Epoch 6/33] [Batch 1200/3147] [D loss: 31934.632812] [G loss: 140.545975] time: 3:53:13.682196 \n",
            "[Epoch 6/33] [Batch 1250/3147] [D loss: 12159.257812] [G loss: 103.491783] time: 3:53:54.127852 \n",
            "[Epoch 6/33] [Batch 1300/3147] [D loss: 11196.456055] [G loss: 92.634476] time: 3:54:34.180787 \n",
            "[Epoch 6/33] [Batch 1350/3147] [D loss: 186261.531250] [G loss: 255.539383] time: 3:55:14.559921 \n",
            "[Epoch 6/33] [Batch 1400/3147] [D loss: 6194.360840] [G loss: 52.686100] time: 3:55:54.499401 \n",
            "[Epoch 6/33] [Batch 1450/3147] [D loss: 1515.095459] [G loss: 49.324257] time: 3:56:34.831301 \n",
            "[Epoch 6/33] [Batch 1500/3147] [D loss: 1441.881104] [G loss: 48.634945] time: 3:57:14.844443 \n",
            "[Epoch 6/33] [Batch 1550/3147] [D loss: 8995.810547] [G loss: 237.386032] time: 3:57:55.214225 \n",
            "[Epoch 6/33] [Batch 1600/3147] [D loss: 15532.836914] [G loss: 160.540131] time: 3:58:35.623189 \n",
            "[Epoch 6/33] [Batch 1650/3147] [D loss: 55085.285156] [G loss: 285.934448] time: 3:59:15.830665 \n",
            "[Epoch 6/33] [Batch 1700/3147] [D loss: 55728.902344] [G loss: 329.274414] time: 3:59:55.687430 \n",
            "[Epoch 6/33] [Batch 1750/3147] [D loss: 36570.949219] [G loss: 324.391510] time: 4:00:35.811653 \n",
            "[Epoch 6/33] [Batch 1800/3147] [D loss: 46253.468750] [G loss: 328.392883] time: 4:01:15.720597 \n",
            "[Epoch 6/33] [Batch 1850/3147] [D loss: 49931.234375] [G loss: 393.898438] time: 4:01:55.928332 \n",
            "[Epoch 6/33] [Batch 1900/3147] [D loss: 106860.929688] [G loss: 509.788452] time: 4:02:35.781821 \n",
            "[Epoch 6/33] [Batch 1950/3147] [D loss: 38201.953125] [G loss: 335.582153] time: 4:03:16.129433 \n",
            "[Epoch 6/33] [Batch 2000/3147] [D loss: 67159.851562] [G loss: 394.566467] time: 4:03:56.271777 \n",
            "[Epoch 6/33] [Batch 2050/3147] [D loss: 58754.441406] [G loss: 324.384827] time: 4:04:41.287208 \n",
            "[Epoch 6/33] [Batch 2100/3147] [D loss: 19206.074219] [G loss: 263.436157] time: 4:05:21.103072 \n",
            "[Epoch 6/33] [Batch 2150/3147] [D loss: 27915.208984] [G loss: 344.925568] time: 4:06:01.269027 \n",
            "[Epoch 6/33] [Batch 2200/3147] [D loss: 58481.824219] [G loss: 402.847321] time: 4:06:41.228549 \n",
            "[Epoch 6/33] [Batch 2250/3147] [D loss: 16282.420898] [G loss: 270.126587] time: 4:07:21.480534 \n",
            "[Epoch 6/33] [Batch 2300/3147] [D loss: 4701.453125] [G loss: 152.579056] time: 4:08:01.528269 \n",
            "[Epoch 6/33] [Batch 2350/3147] [D loss: 7808.318359] [G loss: 163.549500] time: 4:08:42.188646 \n",
            "[Epoch 6/33] [Batch 2400/3147] [D loss: 12599.477539] [G loss: 229.706848] time: 4:09:21.981972 \n",
            "[Epoch 6/33] [Batch 2450/3147] [D loss: 12286.196289] [G loss: 211.191589] time: 4:10:02.206545 \n",
            "[Epoch 6/33] [Batch 2500/3147] [D loss: 2673.427734] [G loss: 130.263916] time: 4:10:42.046554 \n",
            "[Epoch 6/33] [Batch 2550/3147] [D loss: 9910.982422] [G loss: 184.002899] time: 4:11:22.414855 \n",
            "[Epoch 6/33] [Batch 2600/3147] [D loss: 6679.553711] [G loss: 157.647919] time: 4:12:02.293208 \n",
            "[Epoch 6/33] [Batch 2650/3147] [D loss: 2311.098145] [G loss: 114.559219] time: 4:12:42.575751 \n",
            "[Epoch 6/33] [Batch 2700/3147] [D loss: 2379.702881] [G loss: 105.468803] time: 4:13:22.554368 \n",
            "[Epoch 6/33] [Batch 2750/3147] [D loss: 1221.960449] [G loss: 85.180565] time: 4:14:03.173622 \n",
            "[Epoch 6/33] [Batch 2800/3147] [D loss: 1098.301270] [G loss: 70.719749] time: 4:14:43.093099 \n",
            "[Epoch 6/33] [Batch 2850/3147] [D loss: 1269.689941] [G loss: 74.187424] time: 4:15:23.254729 \n",
            "[Epoch 6/33] [Batch 2900/3147] [D loss: 1354.796021] [G loss: 67.400055] time: 4:16:03.001359 \n",
            "[Epoch 6/33] [Batch 2950/3147] [D loss: 581.046082] [G loss: 55.520599] time: 4:16:43.288619 \n",
            "[Epoch 6/33] [Batch 3000/3147] [D loss: 2616.426025] [G loss: 65.763977] time: 4:17:23.093021 \n",
            "[Epoch 6/33] [Batch 3050/3147] [D loss: 477.690247] [G loss: 46.202168] time: 4:18:08.043832 \n",
            "[Epoch 6/33] [Batch 3100/3147] [D loss: 412.261292] [G loss: 43.345215] time: 4:18:48.550201 \n",
            "[Epoch 7/33] [Batch 50/3147] [D loss: 103.801384] [G loss: 28.987549] time: 4:20:06.197438 \n",
            "[Epoch 7/33] [Batch 100/3147] [D loss: 69.855934] [G loss: 21.606134] time: 4:20:46.405605 \n",
            "[Epoch 7/33] [Batch 150/3147] [D loss: 54.332211] [G loss: 23.730984] time: 4:21:26.248591 \n",
            "[Epoch 7/33] [Batch 200/3147] [D loss: 138.747055] [G loss: 24.056736] time: 4:22:06.590845 \n",
            "[Epoch 7/33] [Batch 250/3147] [D loss: 49.386261] [G loss: 19.112707] time: 4:22:46.569829 \n",
            "[Epoch 7/33] [Batch 300/3147] [D loss: 30.756287] [G loss: 17.029091] time: 4:23:26.858367 \n",
            "[Epoch 7/33] [Batch 350/3147] [D loss: 185.757889] [G loss: 18.652744] time: 4:24:07.103942 \n",
            "[Epoch 7/33] [Batch 400/3147] [D loss: 31.513319] [G loss: 13.221319] time: 4:24:47.375096 \n",
            "[Epoch 7/33] [Batch 450/3147] [D loss: 20.399460] [G loss: 14.010598] time: 4:25:27.247416 \n",
            "[Epoch 7/33] [Batch 500/3147] [D loss: 77.380585] [G loss: 12.696321] time: 4:26:07.438587 \n",
            "[Epoch 7/33] [Batch 550/3147] [D loss: 7.655804] [G loss: 9.723466] time: 4:26:47.309468 \n",
            "[Epoch 7/33] [Batch 600/3147] [D loss: 7.135916] [G loss: 8.669357] time: 4:27:27.716242 \n",
            "[Epoch 7/33] [Batch 650/3147] [D loss: 5.353427] [G loss: 9.826362] time: 4:28:07.725218 \n",
            "[Epoch 7/33] [Batch 700/3147] [D loss: 4.206212] [G loss: 7.433943] time: 4:28:48.141526 \n",
            "[Epoch 7/33] [Batch 750/3147] [D loss: 3.992891] [G loss: 6.509955] time: 4:29:28.401943 \n",
            "[Epoch 7/33] [Batch 800/3147] [D loss: 11.530000] [G loss: 7.259245] time: 4:30:08.590818 \n",
            "[Epoch 7/33] [Batch 850/3147] [D loss: 14.303707] [G loss: 7.704435] time: 4:30:48.297613 \n",
            "[Epoch 7/33] [Batch 900/3147] [D loss: 2.986257] [G loss: 5.632548] time: 4:31:28.426972 \n",
            "[Epoch 7/33] [Batch 950/3147] [D loss: 35.273731] [G loss: 6.566556] time: 4:32:08.188052 \n",
            "[Epoch 7/33] [Batch 1000/3147] [D loss: 4.199229] [G loss: 5.773993] time: 4:32:48.346140 \n",
            "[Epoch 7/33] [Batch 1050/3147] [D loss: 19212066.000000] [G loss: 3684.827393] time: 4:33:33.230511 \n",
            "[Epoch 7/33] [Batch 1100/3147] [D loss: 809242.875000] [G loss: 825.064697] time: 4:34:13.332457 \n",
            "[Epoch 7/33] [Batch 1150/3147] [D loss: 196414.656250] [G loss: 408.599060] time: 4:34:53.735086 \n",
            "[Epoch 7/33] [Batch 1200/3147] [D loss: 68837.101562] [G loss: 252.876251] time: 4:35:34.170188 \n",
            "[Epoch 7/33] [Batch 1250/3147] [D loss: 33037.320312] [G loss: 187.545624] time: 4:36:14.260449 \n",
            "[Epoch 7/33] [Batch 1300/3147] [D loss: 17696.644531] [G loss: 143.341812] time: 4:36:54.771911 \n",
            "[Epoch 7/33] [Batch 1350/3147] [D loss: 11056.806641] [G loss: 116.151619] time: 4:37:34.909748 \n",
            "[Epoch 7/33] [Batch 1400/3147] [D loss: 8906.709961] [G loss: 100.635323] time: 4:38:15.422023 \n",
            "[Epoch 7/33] [Batch 1450/3147] [D loss: 8680.296875] [G loss: 85.760635] time: 4:38:55.790140 \n",
            "[Epoch 7/33] [Batch 1500/3147] [D loss: 8554.479492] [G loss: 110.575417] time: 4:39:36.652089 \n",
            "[Epoch 7/33] [Batch 1550/3147] [D loss: 57146.507812] [G loss: 281.067841] time: 4:40:16.814493 \n",
            "[Epoch 7/33] [Batch 1600/3147] [D loss: 220783.796875] [G loss: 423.701996] time: 4:40:57.484124 \n",
            "[Epoch 7/33] [Batch 1650/3147] [D loss: 13296.984375] [G loss: 141.965118] time: 4:41:37.933509 \n",
            "[Epoch 7/33] [Batch 1700/3147] [D loss: 102504.070312] [G loss: 345.298828] time: 4:42:18.472720 \n",
            "[Epoch 7/33] [Batch 1750/3147] [D loss: 480275.875000] [G loss: 1100.911377] time: 4:42:58.560532 \n",
            "[Epoch 7/33] [Batch 1800/3147] [D loss: 494439.375000] [G loss: 925.179565] time: 4:43:39.087058 \n",
            "[Epoch 7/33] [Batch 1850/3147] [D loss: 522716.968750] [G loss: 1210.819458] time: 4:44:19.402243 \n",
            "[Epoch 7/33] [Batch 1900/3147] [D loss: 49553.582031] [G loss: 234.294800] time: 4:45:00.226060 \n",
            "[Epoch 7/33] [Batch 1950/3147] [D loss: 177807.109375] [G loss: 510.092499] time: 4:45:40.513874 \n",
            "[Epoch 7/33] [Batch 2000/3147] [D loss: 114019.875000] [G loss: 500.762787] time: 4:46:21.133521 \n",
            "[Epoch 7/33] [Batch 2050/3147] [D loss: 194668.343750] [G loss: 537.431641] time: 4:47:06.280539 \n",
            "[Epoch 7/33] [Batch 2100/3147] [D loss: 215669.687500] [G loss: 692.740417] time: 4:47:47.076174 \n",
            "[Epoch 7/33] [Batch 2150/3147] [D loss: 150057.625000] [G loss: 331.782562] time: 4:48:27.478996 \n",
            "[Epoch 7/33] [Batch 2200/3147] [D loss: 178303.187500] [G loss: 521.764832] time: 4:49:08.186785 \n",
            "[Epoch 7/33] [Batch 2250/3147] [D loss: 162667.234375] [G loss: 475.975708] time: 4:49:48.665840 \n",
            "[Epoch 7/33] [Batch 2300/3147] [D loss: 160771.375000] [G loss: 567.267212] time: 4:50:29.581195 \n",
            "[Epoch 7/33] [Batch 2350/3147] [D loss: 172179.781250] [G loss: 527.664551] time: 4:51:09.924107 \n",
            "[Epoch 7/33] [Batch 2400/3147] [D loss: 67377.562500] [G loss: 347.692474] time: 4:51:50.477032 \n",
            "[Epoch 7/33] [Batch 2450/3147] [D loss: 23708.681641] [G loss: 158.365601] time: 4:52:30.827395 \n",
            "[Epoch 7/33] [Batch 2500/3147] [D loss: 60447.218750] [G loss: 312.082245] time: 4:53:11.383535 \n",
            "[Epoch 7/33] [Batch 2550/3147] [D loss: 47508.355469] [G loss: 327.302124] time: 4:53:51.879246 \n",
            "[Epoch 7/33] [Batch 2600/3147] [D loss: 56419.351562] [G loss: 304.875549] time: 4:54:32.531597 \n",
            "[Epoch 7/33] [Batch 2650/3147] [D loss: 22228.185547] [G loss: 191.489197] time: 4:55:13.293530 \n",
            "[Epoch 7/33] [Batch 2700/3147] [D loss: 21601.238281] [G loss: 189.224869] time: 4:55:53.863387 \n",
            "[Epoch 7/33] [Batch 2750/3147] [D loss: 21367.224609] [G loss: 181.362656] time: 4:56:34.164804 \n",
            "[Epoch 7/33] [Batch 2800/3147] [D loss: 11380.021484] [G loss: 146.958496] time: 4:57:14.763408 \n",
            "[Epoch 7/33] [Batch 2850/3147] [D loss: 8543.833984] [G loss: 126.411713] time: 4:57:54.949820 \n",
            "[Epoch 7/33] [Batch 2900/3147] [D loss: 8636.881836] [G loss: 121.966583] time: 4:58:35.508396 \n",
            "[Epoch 7/33] [Batch 2950/3147] [D loss: 5961.436035] [G loss: 98.498436] time: 4:59:15.952370 \n",
            "[Epoch 7/33] [Batch 3000/3147] [D loss: 9165.191406] [G loss: 121.999825] time: 4:59:56.743031 \n",
            "[Epoch 7/33] [Batch 3050/3147] [D loss: 3024.910156] [G loss: 71.661713] time: 5:00:42.698273 \n",
            "[Epoch 7/33] [Batch 3100/3147] [D loss: 22847598.000000] [G loss: 4249.012207] time: 5:01:23.643578 \n",
            "[Epoch 8/33] [Batch 50/3147] [D loss: 527204.812500] [G loss: 641.513123] time: 5:02:42.831138 \n",
            "[Epoch 8/33] [Batch 100/3147] [D loss: 162579.093750] [G loss: 405.753418] time: 5:03:23.421128 \n",
            "[Epoch 8/33] [Batch 150/3147] [D loss: 85531.320312] [G loss: 262.685272] time: 5:04:04.517383 \n",
            "[Epoch 8/33] [Batch 200/3147] [D loss: 56428.121094] [G loss: 216.361832] time: 5:04:45.208143 \n",
            "[Epoch 8/33] [Batch 250/3147] [D loss: 29197.447266] [G loss: 155.247955] time: 5:05:26.664850 \n",
            "[Epoch 8/33] [Batch 300/3147] [D loss: 17901.855469] [G loss: 140.421707] time: 5:06:07.353424 \n",
            "[Epoch 8/33] [Batch 350/3147] [D loss: 24610.289062] [G loss: 126.321075] time: 5:06:48.275586 \n",
            "[Epoch 8/33] [Batch 400/3147] [D loss: 85725.000000] [G loss: 208.182312] time: 5:07:28.858480 \n",
            "[Epoch 8/33] [Batch 450/3147] [D loss: 13462.280273] [G loss: 130.534973] time: 5:08:09.687475 \n",
            "[Epoch 8/33] [Batch 500/3147] [D loss: 40247.488281] [G loss: 129.532211] time: 5:08:50.372186 \n",
            "[Epoch 8/33] [Batch 550/3147] [D loss: 2397796.500000] [G loss: 1061.190063] time: 5:09:31.383897 \n",
            "[Epoch 8/33] [Batch 600/3147] [D loss: 1116154.250000] [G loss: 602.641968] time: 5:10:12.197070 \n",
            "[Epoch 8/33] [Batch 650/3147] [D loss: 1272749.875000] [G loss: 779.432373] time: 5:10:53.356139 \n",
            "[Epoch 8/33] [Batch 700/3147] [D loss: 205528.406250] [G loss: 218.116394] time: 5:11:34.262453 \n",
            "[Epoch 8/33] [Batch 750/3147] [D loss: 44449.628906] [G loss: 83.416634] time: 5:12:14.928467 \n",
            "[Epoch 8/33] [Batch 800/3147] [D loss: 95325.429688] [G loss: 253.936478] time: 5:12:55.794296 \n",
            "[Epoch 8/33] [Batch 850/3147] [D loss: 1182933.375000] [G loss: 819.766357] time: 5:13:36.420094 \n",
            "[Epoch 8/33] [Batch 900/3147] [D loss: 167150.265625] [G loss: 326.604523] time: 5:14:17.328871 \n",
            "[Epoch 8/33] [Batch 950/3147] [D loss: 89149.765625] [G loss: 189.643158] time: 5:14:58.033990 \n",
            "[Epoch 8/33] [Batch 1000/3147] [D loss: 848074.000000] [G loss: 839.528198] time: 5:15:38.749592 \n",
            "[Epoch 8/33] [Batch 1050/3147] [D loss: 132359.062500] [G loss: 608.816833] time: 5:16:25.143450 \n",
            "[Epoch 8/33] [Batch 1100/3147] [D loss: 792306.437500] [G loss: 428.124908] time: 5:17:05.779345 \n",
            "[Epoch 8/33] [Batch 1150/3147] [D loss: 73729.757812] [G loss: 271.054321] time: 5:17:46.025432 \n",
            "[Epoch 8/33] [Batch 1200/3147] [D loss: 550070.750000] [G loss: 910.373657] time: 5:18:26.689314 \n",
            "[Epoch 8/33] [Batch 1250/3147] [D loss: 938779.062500] [G loss: 704.327637] time: 5:19:07.037626 \n",
            "[Epoch 8/33] [Batch 1300/3147] [D loss: 28241.640625] [G loss: 323.200836] time: 5:19:47.660444 \n",
            "[Epoch 8/33] [Batch 1350/3147] [D loss: 159103.171875] [G loss: 221.874954] time: 5:20:27.758734 \n",
            "[Epoch 8/33] [Batch 1400/3147] [D loss: 3054493.250000] [G loss: 1282.491699] time: 5:21:08.478964 \n",
            "[Epoch 8/33] [Batch 1450/3147] [D loss: 1065135.000000] [G loss: 649.711243] time: 5:21:48.682784 \n",
            "[Epoch 8/33] [Batch 1500/3147] [D loss: 107698.796875] [G loss: 436.149292] time: 5:22:29.200508 \n",
            "[Epoch 8/33] [Batch 1550/3147] [D loss: 161243.265625] [G loss: 714.995483] time: 5:23:09.413444 \n",
            "[Epoch 8/33] [Batch 1600/3147] [D loss: 1755.366821] [G loss: 52.709805] time: 5:23:49.816354 \n",
            "[Epoch 8/33] [Batch 1650/3147] [D loss: 91771.445312] [G loss: 42.718380] time: 5:24:30.130821 \n",
            "[Epoch 8/33] [Batch 1700/3147] [D loss: 101836.343750] [G loss: 269.009338] time: 5:25:10.608256 \n",
            "[Epoch 8/33] [Batch 1750/3147] [D loss: 5540.331055] [G loss: 140.008972] time: 5:25:50.896257 \n",
            "[Epoch 8/33] [Batch 1800/3147] [D loss: 1139261.250000] [G loss: 1168.104248] time: 5:26:31.729871 \n",
            "[Epoch 8/33] [Batch 1850/3147] [D loss: 297.060883] [G loss: 9.637316] time: 5:27:11.925920 \n",
            "[Epoch 8/33] [Batch 1900/3147] [D loss: 48565.238281] [G loss: 280.941498] time: 5:27:52.504970 \n",
            "[Epoch 8/33] [Batch 1950/3147] [D loss: 40401.312500] [G loss: 373.109955] time: 5:28:32.796522 \n",
            "[Epoch 8/33] [Batch 2000/3147] [D loss: 12316.630859] [G loss: 272.725922] time: 5:29:13.196215 \n",
            "[Epoch 8/33] [Batch 2050/3147] [D loss: 52349.382812] [G loss: 423.057129] time: 5:29:58.445702 \n",
            "[Epoch 8/33] [Batch 2100/3147] [D loss: 97126.382812] [G loss: 272.518555] time: 5:30:38.850880 \n",
            "[Epoch 8/33] [Batch 2150/3147] [D loss: 5686.600098] [G loss: 162.073196] time: 5:31:19.486300 \n",
            "[Epoch 8/33] [Batch 2200/3147] [D loss: 6330.584473] [G loss: 155.821823] time: 5:32:00.312665 \n",
            "[Epoch 8/33] [Batch 2250/3147] [D loss: 2721.908447] [G loss: 116.407997] time: 5:32:40.875548 \n",
            "[Epoch 8/33] [Batch 2300/3147] [D loss: 1357.994141] [G loss: 102.151482] time: 5:33:21.523886 \n",
            "[Epoch 8/33] [Batch 2350/3147] [D loss: 2421.873535] [G loss: 94.046455] time: 5:34:01.951920 \n",
            "[Epoch 8/33] [Batch 2400/3147] [D loss: 1436.855469] [G loss: 84.085663] time: 5:34:42.897865 \n",
            "[Epoch 8/33] [Batch 2450/3147] [D loss: 1271.306641] [G loss: 78.268806] time: 5:35:23.286105 \n",
            "[Epoch 8/33] [Batch 2500/3147] [D loss: 779.929382] [G loss: 66.503479] time: 5:36:03.974931 \n",
            "[Epoch 8/33] [Batch 2550/3147] [D loss: 776.360046] [G loss: 55.869232] time: 5:36:44.896141 \n",
            "[Epoch 8/33] [Batch 2600/3147] [D loss: 554.805115] [G loss: 54.123444] time: 5:37:25.612721 \n",
            "[Epoch 8/33] [Batch 2650/3147] [D loss: 518.565125] [G loss: 48.567478] time: 5:38:06.089388 \n",
            "[Epoch 8/33] [Batch 2700/3147] [D loss: 630.490601] [G loss: 43.488491] time: 5:38:46.892995 \n",
            "[Epoch 8/33] [Batch 2750/3147] [D loss: 168.219620] [G loss: 36.146957] time: 5:39:27.352979 \n",
            "[Epoch 8/33] [Batch 2800/3147] [D loss: 181.152603] [G loss: 33.989922] time: 5:40:08.099437 \n",
            "[Epoch 8/33] [Batch 2850/3147] [D loss: 1168.938354] [G loss: 37.175552] time: 5:40:48.483679 \n",
            "[Epoch 8/33] [Batch 2900/3147] [D loss: 123.612167] [G loss: 27.285263] time: 5:41:29.155688 \n",
            "[Epoch 8/33] [Batch 2950/3147] [D loss: 107.739838] [G loss: 24.381300] time: 5:42:09.930336 \n",
            "[Epoch 8/33] [Batch 3000/3147] [D loss: 88.386276] [G loss: 21.854750] time: 5:42:50.643003 \n",
            "[Epoch 8/33] [Batch 3050/3147] [D loss: 410.481598] [G loss: 29.771614] time: 5:43:37.557664 \n",
            "[Epoch 8/33] [Batch 3100/3147] [D loss: 49.968224] [G loss: 15.426576] time: 5:44:18.194997 \n",
            "[Epoch 9/33] [Batch 50/3147] [D loss: 30.724394] [G loss: 14.100463] time: 5:45:37.036711 \n",
            "[Epoch 9/33] [Batch 100/3147] [D loss: 136.396774] [G loss: 17.394825] time: 5:46:17.342785 \n",
            "[Epoch 9/33] [Batch 150/3147] [D loss: 22.864002] [G loss: 12.307702] time: 5:46:58.503752 \n",
            "[Epoch 9/33] [Batch 200/3147] [D loss: 45.612335] [G loss: 13.697240] time: 5:47:38.812765 \n",
            "[Epoch 9/33] [Batch 250/3147] [D loss: 179410796544.000000] [G loss: 157252.968750] time: 5:48:19.499152 \n",
            "[Epoch 9/33] [Batch 300/3147] [D loss: 12749931.000000] [G loss: 3254.409912] time: 5:48:59.964182 \n",
            "[Epoch 9/33] [Batch 350/3147] [D loss: 2436989.000000] [G loss: 1286.930664] time: 5:49:40.666438 \n",
            "[Epoch 9/33] [Batch 400/3147] [D loss: 802416.250000] [G loss: 767.811829] time: 5:50:21.413604 \n",
            "[Epoch 9/33] [Batch 450/3147] [D loss: 389874.187500] [G loss: 549.687927] time: 5:51:02.154132 \n",
            "[Epoch 9/33] [Batch 500/3147] [D loss: 314451.656250] [G loss: 476.579468] time: 5:51:42.672055 \n",
            "[Epoch 9/33] [Batch 550/3147] [D loss: 1274465.625000] [G loss: 886.190491] time: 5:52:23.807455 \n",
            "[Epoch 9/33] [Batch 600/3147] [D loss: 1563189.000000] [G loss: 1154.890259] time: 5:53:04.483230 \n",
            "[Epoch 9/33] [Batch 650/3147] [D loss: 3751173.750000] [G loss: 2015.247803] time: 5:53:45.063339 \n",
            "[Epoch 9/33] [Batch 700/3147] [D loss: 5895057.000000] [G loss: 2942.435059] time: 5:54:25.680995 \n",
            "[Epoch 9/33] [Batch 750/3147] [D loss: 3754160.000000] [G loss: 1456.656250] time: 5:55:06.611234 \n",
            "[Epoch 9/33] [Batch 800/3147] [D loss: 5632998.500000] [G loss: 3155.107666] time: 5:55:47.362068 \n",
            "[Epoch 9/33] [Batch 850/3147] [D loss: 5130414.000000] [G loss: 3270.199707] time: 5:56:28.095041 \n",
            "[Epoch 9/33] [Batch 900/3147] [D loss: 1435131.000000] [G loss: 214.183502] time: 5:57:08.872587 \n",
            "[Epoch 9/33] [Batch 950/3147] [D loss: 33570812.000000] [G loss: 7351.843750] time: 5:57:49.582701 \n",
            "[Epoch 9/33] [Batch 1000/3147] [D loss: 17282.529297] [G loss: 66.232681] time: 5:58:30.327113 \n",
            "[Epoch 9/33] [Batch 1050/3147] [D loss: 147630800.000000] [G loss: 15372.556641] time: 5:59:16.060017 \n",
            "[Epoch 9/33] [Batch 1100/3147] [D loss: 248145.843750] [G loss: 795.610291] time: 5:59:56.901261 \n",
            "[Epoch 9/33] [Batch 1150/3147] [D loss: 4070707.000000] [G loss: 783.716675] time: 6:00:37.637929 \n",
            "[Epoch 9/33] [Batch 1200/3147] [D loss: 8241948.000000] [G loss: 5116.244629] time: 6:01:18.516402 \n",
            "[Epoch 9/33] [Batch 1250/3147] [D loss: 105748.320312] [G loss: 117.086433] time: 6:01:58.984811 \n",
            "[Epoch 9/33] [Batch 1300/3147] [D loss: 38774692.000000] [G loss: 8301.899414] time: 6:02:40.108690 \n",
            "[Epoch 9/33] [Batch 1350/3147] [D loss: 88590.656250] [G loss: 268.868713] time: 6:03:20.338608 \n",
            "[Epoch 9/33] [Batch 1400/3147] [D loss: 41257824.000000] [G loss: 7635.437500] time: 6:04:00.974989 \n",
            "[Epoch 9/33] [Batch 1450/3147] [D loss: 1086313.875000] [G loss: 869.288269] time: 6:04:41.182797 \n",
            "[Epoch 9/33] [Batch 1500/3147] [D loss: 606038.437500] [G loss: 124.980232] time: 6:05:21.681035 \n",
            "[Epoch 9/33] [Batch 1550/3147] [D loss: 7173846.000000] [G loss: 2185.291992] time: 6:06:01.767767 \n",
            "[Epoch 9/33] [Batch 1600/3147] [D loss: 13467.683594] [G loss: 100.851166] time: 6:06:42.143137 \n",
            "[Epoch 9/33] [Batch 1650/3147] [D loss: 1059500.500000] [G loss: 666.081116] time: 6:07:22.172523 \n",
            "[Epoch 9/33] [Batch 1700/3147] [D loss: 1259803.500000] [G loss: 1663.270752] time: 6:08:02.903489 \n",
            "[Epoch 9/33] [Batch 1750/3147] [D loss: 148773.000000] [G loss: 213.440689] time: 6:08:43.104676 \n",
            "[Epoch 9/33] [Batch 1800/3147] [D loss: 4067999.000000] [G loss: 2507.409180] time: 6:09:23.586532 \n",
            "[Epoch 9/33] [Batch 1850/3147] [D loss: 243521.500000] [G loss: 659.349060] time: 6:10:03.766169 \n",
            "[Epoch 9/33] [Batch 1900/3147] [D loss: 330288.937500] [G loss: 463.728546] time: 6:10:44.788441 \n",
            "[Epoch 9/33] [Batch 1950/3147] [D loss: 893683.250000] [G loss: 1493.610107] time: 6:11:25.338925 \n",
            "[Epoch 9/33] [Batch 2000/3147] [D loss: 297282.281250] [G loss: 915.140503] time: 6:12:06.226067 \n",
            "[Epoch 9/33] [Batch 2050/3147] [D loss: 178020.687500] [G loss: 486.254730] time: 6:12:52.057734 \n",
            "[Epoch 9/33] [Batch 2100/3147] [D loss: 286039.500000] [G loss: 590.937134] time: 6:13:32.845538 \n",
            "[Epoch 9/33] [Batch 2150/3147] [D loss: 209601.812500] [G loss: 578.140930] time: 6:14:13.343806 \n",
            "[Epoch 9/33] [Batch 2200/3147] [D loss: 91924.375000] [G loss: 354.478699] time: 6:14:54.109736 \n",
            "[Epoch 9/33] [Batch 2250/3147] [D loss: 156370.109375] [G loss: 464.872406] time: 6:15:34.604118 \n",
            "[Epoch 9/33] [Batch 2300/3147] [D loss: 67285.046875] [G loss: 316.196014] time: 6:16:15.238427 \n",
            "[Epoch 9/33] [Batch 2350/3147] [D loss: 32382.013672] [G loss: 240.923431] time: 6:16:55.681659 \n",
            "[Epoch 9/33] [Batch 2400/3147] [D loss: 40401.363281] [G loss: 258.630096] time: 6:17:36.465303 \n",
            "[Epoch 9/33] [Batch 2450/3147] [D loss: 31770.091797] [G loss: 211.326874] time: 6:18:17.341828 \n",
            "[Epoch 9/33] [Batch 2500/3147] [D loss: 12729.069336] [G loss: 112.150902] time: 6:18:58.310516 \n",
            "[Epoch 9/33] [Batch 2550/3147] [D loss: 15514.566406] [G loss: 228.951096] time: 6:19:38.849757 \n",
            "[Epoch 9/33] [Batch 2600/3147] [D loss: 26465.818359] [G loss: 223.231247] time: 6:20:19.732486 \n",
            "[Epoch 9/33] [Batch 2650/3147] [D loss: 11833.890625] [G loss: 141.386322] time: 6:21:00.340187 \n",
            "[Epoch 9/33] [Batch 2700/3147] [D loss: 17890.250000] [G loss: 166.586090] time: 6:21:41.116624 \n",
            "[Epoch 9/33] [Batch 2750/3147] [D loss: 14863.363281] [G loss: 149.770370] time: 6:22:21.569000 \n",
            "[Epoch 9/33] [Batch 2800/3147] [D loss: 5770.015625] [G loss: 104.094109] time: 6:23:02.634797 \n",
            "[Epoch 9/33] [Batch 2850/3147] [D loss: 5710.705078] [G loss: 106.928322] time: 6:23:43.250669 \n",
            "[Epoch 9/33] [Batch 2900/3147] [D loss: 3641.412598] [G loss: 82.638443] time: 6:24:24.041788 \n",
            "[Epoch 9/33] [Batch 2950/3147] [D loss: 2616.754150] [G loss: 89.726357] time: 6:25:04.542798 \n",
            "[Epoch 9/33] [Batch 3000/3147] [D loss: 2287716.250000] [G loss: 1377.674072] time: 6:25:45.628875 \n",
            "[Epoch 9/33] [Batch 3050/3147] [D loss: 331004.843750] [G loss: 552.483276] time: 6:26:31.212212 \n",
            "[Epoch 9/33] [Batch 3100/3147] [D loss: 283578.781250] [G loss: 749.641663] time: 6:27:11.980226 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0516 20:09:25.593103 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 20:09:33.289025 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 20:09:40.227596 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 20:09:40.259601 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 20:09:40.287932 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0516 20:09:40.899612 140399260288896 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 10/33] [Batch 50/3147] [D loss: 12660886315833569050624.000000] [G loss: 155324465152.000000] time: 6:29:46.007151 \n",
            "[Epoch 10/33] [Batch 100/3147] [D loss: 2694026275495266484224.000000] [G loss: 64172572672.000000] time: 6:30:23.042907 \n",
            "[Epoch 10/33] [Batch 150/3147] [D loss: 887765180408479612928.000000] [G loss: 36669710336.000000] time: 6:31:00.326064 \n",
            "[Epoch 10/33] [Batch 200/3147] [D loss: 295283752039987806208.000000] [G loss: 24213409792.000000] time: 6:31:37.279669 \n",
            "[Epoch 10/33] [Batch 250/3147] [D loss: 229195608939395284992.000000] [G loss: 17296392192.000000] time: 6:32:14.447362 \n",
            "[Epoch 10/33] [Batch 300/3147] [D loss: 173458989582313848832.000000] [G loss: 13201119232.000000] time: 6:32:51.405659 \n",
            "[Epoch 10/33] [Batch 350/3147] [D loss: 164869235310220804096.000000] [G loss: 10285912064.000000] time: 6:33:28.870179 \n",
            "[Epoch 10/33] [Batch 400/3147] [D loss: 33970733353116106752.000000] [G loss: 8101638656.000000] time: 6:34:05.796412 \n",
            "[Epoch 10/33] [Batch 450/3147] [D loss: 31116414361549144064.000000] [G loss: 6656092160.000000] time: 6:34:43.028476 \n",
            "[Epoch 10/33] [Batch 500/3147] [D loss: 24000093631606161408.000000] [G loss: 5567136768.000000] time: 6:35:19.951784 \n",
            "[Epoch 10/33] [Batch 550/3147] [D loss: 56732354679381426176.000000] [G loss: 4832931328.000000] time: 6:35:57.088047 \n",
            "[Epoch 10/33] [Batch 600/3147] [D loss: 21377169061127913472.000000] [G loss: 3948360704.000000] time: 6:36:34.169578 \n",
            "[Epoch 10/33] [Batch 650/3147] [D loss: 18302760827029028864.000000] [G loss: 3428530688.000000] time: 6:37:10.868875 \n",
            "[Epoch 10/33] [Batch 700/3147] [D loss: 21613089072076554240.000000] [G loss: 2925523200.000000] time: 6:37:47.895743 \n",
            "[Epoch 10/33] [Batch 750/3147] [D loss: 7864360821516664832.000000] [G loss: 2665867264.000000] time: 6:38:24.595121 \n",
            "[Epoch 10/33] [Batch 800/3147] [D loss: 5975851194520698880.000000] [G loss: 2312937216.000000] time: 6:39:01.786220 \n",
            "[Epoch 10/33] [Batch 850/3147] [D loss: 5175555163569520640.000000] [G loss: 1955764480.000000] time: 6:39:38.436528 \n",
            "[Epoch 10/33] [Batch 900/3147] [D loss: 4365615591009026048.000000] [G loss: 1757602304.000000] time: 6:40:15.346707 \n",
            "[Epoch 10/33] [Batch 950/3147] [D loss: 6958214256403677184.000000] [G loss: 1573309184.000000] time: 6:40:51.989455 \n",
            "[Epoch 10/33] [Batch 1000/3147] [D loss: 2073645434463307431936.000000] [G loss: 25681940480.000000] time: 6:41:28.875747 \n",
            "[Epoch 10/33] [Batch 1050/3147] [D loss: 2832031341116129280.000000] [G loss: 1390165376.000000] time: 6:42:11.296376 \n",
            "[Epoch 10/33] [Batch 1100/3147] [D loss: 2057646538624073728.000000] [G loss: 1367651328.000000] time: 6:42:48.295410 \n",
            "[Epoch 10/33] [Batch 1150/3147] [D loss: 6417189114596032512.000000] [G loss: 1131632640.000000] time: 6:43:24.779747 \n",
            "[Epoch 10/33] [Batch 1200/3147] [D loss: 22584734094611972096.000000] [G loss: 2843867904.000000] time: 6:44:01.974807 \n",
            "[Epoch 10/33] [Batch 1250/3147] [D loss: 55979457495192043520.000000] [G loss: 5169257472.000000] time: 6:44:38.861704 \n",
            "[Epoch 10/33] [Batch 1300/3147] [D loss: 6453492195785661480960.000000] [G loss: 52865503232.000000] time: 6:45:15.573165 \n",
            "[Epoch 10/33] [Batch 1350/3147] [D loss: 43796508019620052992.000000] [G loss: 4432311296.000000] time: 6:45:52.645876 \n",
            "[Epoch 10/33] [Batch 1400/3147] [D loss: 2514895881791581716480.000000] [G loss: 41548984320.000000] time: 6:46:29.319872 \n",
            "[Epoch 10/33] [Batch 1450/3147] [D loss: 75766668970881449984.000000] [G loss: 5022984192.000000] time: 6:47:06.277220 \n",
            "[Epoch 10/33] [Batch 1500/3147] [D loss: 32304302534942523392.000000] [G loss: 4004103168.000000] time: 6:47:42.869831 \n",
            "[Epoch 10/33] [Batch 1550/3147] [D loss: 7014848437888837746688.000000] [G loss: 52546494464.000000] time: 6:48:19.845671 \n",
            "[Epoch 10/33] [Batch 1600/3147] [D loss: 181220538919807877120.000000] [G loss: 7826387968.000000] time: 6:48:56.777685 \n",
            "[Epoch 10/33] [Batch 1650/3147] [D loss: 5481897997075725942784.000000] [G loss: 41157562368.000000] time: 6:49:33.782634 \n",
            "[Epoch 10/33] [Batch 1700/3147] [D loss: 4576763055228059648.000000] [G loss: 1819271936.000000] time: 6:50:10.391958 \n",
            "[Epoch 10/33] [Batch 1750/3147] [D loss: 13536793659160670502912.000000] [G loss: 70464241664.000000] time: 6:50:47.166551 \n",
            "[Epoch 10/33] [Batch 1800/3147] [D loss: 3583125788030842961920.000000] [G loss: 37692104704.000000] time: 6:51:24.115787 \n",
            "[Epoch 10/33] [Batch 1850/3147] [D loss: 88098017331831111680.000000] [G loss: 5309702144.000000] time: 6:52:00.781180 \n",
            "[Epoch 10/33] [Batch 1900/3147] [D loss: 337877380594925568.000000] [G loss: 519948896.000000] time: 6:52:37.636535 \n",
            "[Epoch 10/33] [Batch 1950/3147] [D loss: 5055518730386145280.000000] [G loss: 1435829760.000000] time: 6:53:14.293773 \n",
            "[Epoch 10/33] [Batch 2000/3147] [D loss: 7116309734826704896.000000] [G loss: 1666428544.000000] time: 6:53:51.175428 \n",
            "[Epoch 10/33] [Batch 2050/3147] [D loss: 111247830034375770112.000000] [G loss: 5760507904.000000] time: 6:54:33.635741 \n",
            "[Epoch 10/33] [Batch 2100/3147] [D loss: 36756697905807491072.000000] [G loss: 3924186368.000000] time: 6:55:10.524024 \n",
            "[Epoch 10/33] [Batch 2150/3147] [D loss: 285992702863420162048.000000] [G loss: 11461947392.000000] time: 6:55:47.054012 \n",
            "[Epoch 10/33] [Batch 2200/3147] [D loss: 394790715437994737664.000000] [G loss: 11350304768.000000] time: 6:56:24.158446 \n",
            "[Epoch 10/33] [Batch 2250/3147] [D loss: 9442314488758927360.000000] [G loss: 2102638848.000000] time: 6:57:00.823125 \n",
            "[Epoch 10/33] [Batch 2300/3147] [D loss: 1208710312283340800.000000] [G loss: 753750144.000000] time: 6:57:37.712433 \n",
            "[Epoch 10/33] [Batch 2350/3147] [D loss: 21176686309942493184.000000] [G loss: 3465321984.000000] time: 6:58:14.378213 \n",
            "[Epoch 10/33] [Batch 2400/3147] [D loss: 30358484558020608.000000] [G loss: 180498272.000000] time: 6:58:51.231524 \n",
            "[Epoch 10/33] [Batch 2450/3147] [D loss: 374520294990176845824.000000] [G loss: 12423327744.000000] time: 6:59:28.134210 \n",
            "[Epoch 10/33] [Batch 2500/3147] [D loss: 532350105015654809600.000000] [G loss: 19798452224.000000] time: 7:00:04.853334 \n",
            "[Epoch 10/33] [Batch 2550/3147] [D loss: 21868718928464707584.000000] [G loss: 3555781632.000000] time: 7:00:41.607194 \n",
            "[Epoch 10/33] [Batch 2600/3147] [D loss: 63601667923121274880.000000] [G loss: 6220093440.000000] time: 7:01:18.408546 \n",
            "[Epoch 10/33] [Batch 2650/3147] [D loss: 179869546992526950400.000000] [G loss: 6814624768.000000] time: 7:01:55.291206 \n",
            "[Epoch 10/33] [Batch 2700/3147] [D loss: 681798226851921920.000000] [G loss: 573177472.000000] time: 7:02:31.881053 \n",
            "[Epoch 10/33] [Batch 2750/3147] [D loss: 4177345390351220736.000000] [G loss: 1300873600.000000] time: 7:03:08.732002 \n",
            "[Epoch 10/33] [Batch 2800/3147] [D loss: 182488354991470804992.000000] [G loss: 8499452928.000000] time: 7:03:45.241846 \n",
            "[Epoch 10/33] [Batch 2850/3147] [D loss: 173750984686279065600.000000] [G loss: 11908106240.000000] time: 7:04:22.237767 \n",
            "[Epoch 10/33] [Batch 2900/3147] [D loss: 235513860149433335808.000000] [G loss: 9042132992.000000] time: 7:04:59.212555 \n",
            "[Epoch 10/33] [Batch 2950/3147] [D loss: 7152302247961952256.000000] [G loss: 1618678656.000000] time: 7:05:36.313582 \n",
            "[Epoch 10/33] [Batch 3000/3147] [D loss: 5146887596898516992.000000] [G loss: 1424010496.000000] time: 7:06:13.196026 \n",
            "[Epoch 10/33] [Batch 3050/3147] [D loss: 266784903229243129856.000000] [G loss: 10018441216.000000] time: 7:06:56.337009 \n",
            "[Epoch 10/33] [Batch 3100/3147] [D loss: 6979302339668606976.000000] [G loss: 2282147328.000000] time: 7:07:33.153356 \n",
            "[Epoch 11/33] [Batch 50/3147] [D loss: 1888779831978491904.000000] [G loss: 1192463872.000000] time: 7:08:44.720333 \n",
            "[Epoch 11/33] [Batch 100/3147] [D loss: 4786326398069374976.000000] [G loss: 1714725376.000000] time: 7:09:21.541147 \n",
            "[Epoch 11/33] [Batch 150/3147] [D loss: 443593620956274229248.000000] [G loss: 12224207872.000000] time: 7:09:58.759229 \n",
            "[Epoch 11/33] [Batch 200/3147] [D loss: 35534079063490560.000000] [G loss: 109986064.000000] time: 7:10:35.553734 \n",
            "[Epoch 11/33] [Batch 250/3147] [D loss: 9235727249016094720.000000] [G loss: 1781797888.000000] time: 7:11:12.716179 \n",
            "[Epoch 11/33] [Batch 300/3147] [D loss: 4247670703819587584.000000] [G loss: 2630967808.000000] time: 7:11:49.649330 \n",
            "[Epoch 11/33] [Batch 350/3147] [D loss: 274653245794680832.000000] [G loss: 335143712.000000] time: 7:12:26.688481 \n",
            "[Epoch 11/33] [Batch 400/3147] [D loss: 51301585274819051520.000000] [G loss: 4761464832.000000] time: 7:13:03.539485 \n",
            "[Epoch 11/33] [Batch 450/3147] [D loss: 558886330243743744.000000] [G loss: 428924672.000000] time: 7:13:40.571903 \n",
            "[Epoch 11/33] [Batch 500/3147] [D loss: 614605422011088896.000000] [G loss: 743677632.000000] time: 7:14:17.243513 \n",
            "[Epoch 11/33] [Batch 550/3147] [D loss: 1621416949571387392.000000] [G loss: 690697024.000000] time: 7:14:54.548006 \n",
            "[Epoch 11/33] [Batch 600/3147] [D loss: 5481100099079634944.000000] [G loss: 2006637568.000000] time: 7:15:31.393004 \n",
            "[Epoch 11/33] [Batch 650/3147] [D loss: 854276623036317696.000000] [G loss: 605864704.000000] time: 7:16:08.251225 \n",
            "[Epoch 11/33] [Batch 700/3147] [D loss: 931378845141958656.000000] [G loss: 576965056.000000] time: 7:16:45.389724 \n",
            "[Epoch 11/33] [Batch 750/3147] [D loss: 462592201394749440.000000] [G loss: 388322688.000000] time: 7:17:22.045914 \n",
            "[Epoch 11/33] [Batch 800/3147] [D loss: 15357494631658946560.000000] [G loss: 2999784960.000000] time: 7:17:59.004435 \n",
            "[Epoch 11/33] [Batch 850/3147] [D loss: 1752822470424920064.000000] [G loss: 784507520.000000] time: 7:18:35.733438 \n",
            "[Epoch 11/33] [Batch 900/3147] [D loss: 1892687221425700864.000000] [G loss: 818887808.000000] time: 7:19:12.809534 \n",
            "[Epoch 11/33] [Batch 950/3147] [D loss: 1144126442376790016.000000] [G loss: 931165440.000000] time: 7:19:49.550597 \n",
            "[Epoch 11/33] [Batch 1000/3147] [D loss: 18105342294491136.000000] [G loss: 102637288.000000] time: 7:20:26.908120 \n",
            "[Epoch 11/33] [Batch 1050/3147] [D loss: 593194700802883584.000000] [G loss: 544421440.000000] time: 7:21:09.876369 \n",
            "[Epoch 11/33] [Batch 1100/3147] [D loss: 319722278956826624.000000] [G loss: 402166336.000000] time: 7:21:46.971462 \n",
            "[Epoch 11/33] [Batch 1150/3147] [D loss: 1307705116568059904.000000] [G loss: 673065408.000000] time: 7:22:23.659976 \n",
            "[Epoch 11/33] [Batch 1200/3147] [D loss: 1040992526569308160.000000] [G loss: 813657152.000000] time: 7:23:00.767079 \n",
            "[Epoch 11/33] [Batch 1250/3147] [D loss: 80916994878078976.000000] [G loss: 287736448.000000] time: 7:23:37.360830 \n",
            "[Epoch 11/33] [Batch 1300/3147] [D loss: 360933383515144192.000000] [G loss: 567024384.000000] time: 7:24:14.208148 \n",
            "[Epoch 11/33] [Batch 1350/3147] [D loss: 451574545128620032.000000] [G loss: 458387904.000000] time: 7:24:51.045514 \n",
            "[Epoch 11/33] [Batch 1400/3147] [D loss: 325750763772968960.000000] [G loss: 420583488.000000] time: 7:25:28.198999 \n",
            "[Epoch 11/33] [Batch 1450/3147] [D loss: 1251920432060170240.000000] [G loss: 687323136.000000] time: 7:26:05.264602 \n",
            "[Epoch 11/33] [Batch 1500/3147] [D loss: 335430898503647232.000000] [G loss: 421105664.000000] time: 7:26:42.149320 \n",
            "[Epoch 11/33] [Batch 1550/3147] [D loss: 33576112994910208.000000] [G loss: 106843408.000000] time: 7:27:19.057188 \n",
            "[Epoch 11/33] [Batch 1600/3147] [D loss: 78378153810067456.000000] [G loss: 196164176.000000] time: 7:27:55.750496 \n",
            "[Epoch 11/33] [Batch 1650/3147] [D loss: 28569420225839104.000000] [G loss: 168716576.000000] time: 7:28:32.782467 \n",
            "[Epoch 11/33] [Batch 1700/3147] [D loss: 9838047793250304.000000] [G loss: 78564432.000000] time: 7:29:09.494016 \n",
            "[Epoch 11/33] [Batch 1750/3147] [D loss: 61936065518239744.000000] [G loss: 205893600.000000] time: 7:29:46.426850 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-940a6d808c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-7c0637bdcc5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sample_interval)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;31m# IMAGE TRANSLATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# genB = genA2B(fgA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mfakeA_fg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgB_fg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0mfakeB_fg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgA_fg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1165\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3217\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m-> 3219\u001b[0;31m                                  [x.numpy() for x in outputs])\n\u001b[0m\u001b[1;32m   3220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3217\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m-> 3219\u001b[0;31m                                  [x.numpy() for x in outputs])\n\u001b[0m\u001b[1;32m   3220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resource handles are not convertible to numpy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cpu_nograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;31m# __int__, __float__ and __index__ may copy the tensor to CPU and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_cpu_nograd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mCPU\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0mbacked\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \"\"\"\n\u001b[0;32m--> 899\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_nograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CPU:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy_nograd\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMUU_eMvDzmf",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNNU8PPNXsGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "64a8218d-46ad-4c0f-da41-7b796e66c9d9"
      },
      "source": [
        "! zip -r samples.zip output/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: output/ (stored 0%)\n",
            "  adding: output/sim2larvae/ (stored 0%)\n",
            "  adding: output/sim2larvae/att004_3.png (deflated 21%)\n",
            "  adding: output/sim2larvae/att009_3.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att005_3.png (deflated 25%)\n",
            "  adding: output/sim2larvae/att005_1.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att006_1.png (deflated 27%)\n",
            "  adding: output/sim2larvae/att008_2.png (deflated 31%)\n",
            "  adding: output/sim2larvae/att006_3.png (deflated 34%)\n",
            "  adding: output/sim2larvae/att008_1.png (deflated 23%)\n",
            "  adding: output/sim2larvae/att004_1.png (deflated 24%)\n",
            "  adding: output/sim2larvae/att002_3.png (deflated 11%)\n",
            "  adding: output/sim2larvae/att010_1.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att005_2.png (deflated 27%)\n",
            "  adding: output/sim2larvae/att001_1.png (deflated 9%)\n",
            "  adding: output/sim2larvae/att010_2.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att009_2.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att003_3.png (deflated 21%)\n",
            "  adding: output/sim2larvae/att009_1.png (deflated 29%)\n",
            "  adding: output/sim2larvae/att003_1.png (deflated 16%)\n",
            "  adding: output/sim2larvae/att007_3.png (deflated 24%)\n",
            "  adding: output/sim2larvae/att002_2.png (deflated 10%)\n",
            "  adding: output/sim2larvae/att004_2.png (deflated 18%)\n",
            "  adding: output/sim2larvae/att007_1.png (deflated 34%)\n",
            "  adding: output/sim2larvae/att001_3.png (deflated 9%)\n",
            "  adding: output/sim2larvae/att010_3.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att007_2.png (deflated 25%)\n",
            "  adding: output/sim2larvae/att011_1.png (deflated 26%)\n",
            "  adding: output/sim2larvae/att008_3.png (deflated 34%)\n",
            "  adding: output/sim2larvae/att003_2.png (deflated 15%)\n",
            "  adding: output/sim2larvae/att006_2.png (deflated 33%)\n",
            "  adding: output/sim2larvae/att002_1.png (deflated 10%)\n",
            "  adding: output/sim2larvae/att001_2.png (deflated 9%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBFBRHDaXbD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# path = 'model/horse2zebra/att_resize_conv/'\n",
        "# # model.load_weights(path + 'combined115', path + 'combined215', path + 'disA15', path + 'disB15')\n",
        "# model.combined_model1.load_weights('weights1.h5')\n",
        "genA = model.genA\n",
        "genB = model.genB\n",
        "attA = model.attA\n",
        "attB = model.attB\n",
        "imgA, imgB = model.data.sample_batch()\n",
        "\n",
        "\n",
        "\n",
        "show_flow(genA, imgB)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VJFUemJTnia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# data = Data_Loader(name='horse2zebra', batch_size=1, patch=None)\n",
        "# imgA, imgB = data.sample_batch()\n",
        "# att = build_attention_net((256,256,3), 'a')\n",
        "\n",
        "\n",
        "# outputs = [layer.output for layer in att.layers]\n",
        "# activation_model = tf.keras.Model(inputs=att.input, outputs=outputs)\n",
        "\n",
        "# preds = activation_model.predict(imgA)\n",
        "# # preds = np.array(preds)\n",
        "# for pred in preds:\n",
        "#     print(pred.shape)\n",
        "\n",
        "\n",
        "# cnt = 16\n",
        "# for pred in preds[23:]:\n",
        "#     print(pred.shape[3])\n",
        "#     print(pred.shape[1:3])\n",
        "#     fig, axs = plt.subplots(pred.shape[3], 1, figsize=pred.shape[1:3])\n",
        "#     fig.tight_layout()\n",
        "#     for j in range(pred.shape[3]):\n",
        "#         img = np.reshape(pred[:,:,:,j], pred.shape[1:3])\n",
        "# #         print(img.shape)\n",
        "#         axs[j].imshow(img, cmap='Greys')\n",
        "# #         axs[i, j].set_title(titles[j])\n",
        "#         axs[j].axis('off')\n",
        "#     fig.savefig(str(cnt) + '.png')\n",
        "#     print('saved ' + str(cnt))\n",
        "#     cnt += 1\n",
        "        \n",
        "# fig.savefig(\"test.png\")\n",
        "# plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZosT7KdCFPp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !zip -r /content/att_origCode_resizeConv.zip /content/model/horse2zebra/att_resize_conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S3lQhjgMtVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.attA.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvsjPLr77W8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! zip -r zp.zip /content/layer_imgs/genA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpi8eP-ZHas8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I40DMTN7EGl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}